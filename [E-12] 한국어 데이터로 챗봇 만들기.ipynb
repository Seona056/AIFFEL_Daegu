{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[E-12] 한국어 데이터로 챗봇 만들기.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1_5cWMjqXD8KPKyPf-HRgpAnBx1lTUd2q","authorship_tag":"ABX9TyNRup17K+EbPG3t1+znqs4U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 12-14. 프로젝트: 한국어 데이터로 챗봇 만들기\n","\n","영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜봅시다."],"metadata":{"id":"iqVwiVqw9tLh"}},{"cell_type":"markdown","source":["# Step 1. 데이터 수집하기\n","\n","한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n","\n","<br>\n","\n","이 데이터는 아래의 링크에서 다운로드할 수 있습니다.\n","\n","<br>\n","\n","- [songys/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)\n","\n","<br>\n","\n","`Cloud shell`에서 아래 명령어를 입력해 주세요.\n","\n","<br>\n","\n","```\n","$ mkdir -p ~/aiffel/transformer_chatbot/data/\n","$ ln -s ~/data/* ~/aiffel/transformer_chatbot/data/\n","```"],"metadata":{"id":"ZlgpVli79v7z"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6X2W89CzZkdf","executionInfo":{"status":"ok","timestamp":1645565759170,"user_tz":-540,"elapsed":3835,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"3ab256a4-0aef-4867-fa31-eb0d42154aa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"juEZR9G5dnhH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 필요한 라이브러리 import"],"metadata":{"id":"niN5TT51d0os"}},{"cell_type":"code","source":["import os\n","import re\n","import csv\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import tensorflow_datasets as tfds"],"metadata":{"id":"pBXjocdKk1pT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_path = '/content/drive/MyDrive/아이펠/풀잎스쿨/data/transformer_chatbot/data/ChatbotData .csv'\n","chatbot = pd.read_csv(dataset_path)\n","chatbot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"Y0L9W8ACdnu5","executionInfo":{"status":"ok","timestamp":1645565762043,"user_tz":-540,"elapsed":97,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"10c991a6-7f5b-4655-8ca5-ed23870d38e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-f9e5f215-8936-4b9a-872e-483a772253ba\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Q</th>\n","      <th>A</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12시 땡!</td>\n","      <td>하루가 또 가네요.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1지망 학교 떨어졌어</td>\n","      <td>위로해 드립니다.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3박4일 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3박4일 정도 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>PPL 심하네</td>\n","      <td>눈살이 찌푸려지죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11818</th>\n","      <td>훔쳐보는 것도 눈치 보임.</td>\n","      <td>티가 나니까 눈치가 보이는 거죠!</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11819</th>\n","      <td>훔쳐보는 것도 눈치 보임.</td>\n","      <td>훔쳐보는 거 티나나봐요.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11820</th>\n","      <td>흑기사 해주는 짝남.</td>\n","      <td>설렜겠어요.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11821</th>\n","      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n","      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11822</th>\n","      <td>힘들어서 결혼할까봐</td>\n","      <td>도피성 결혼은 하지 않길 바라요.</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11823 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9e5f215-8936-4b9a-872e-483a772253ba')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f9e5f215-8936-4b9a-872e-483a772253ba button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f9e5f215-8936-4b9a-872e-483a772253ba');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                             Q                         A  label\n","0                       12시 땡!                하루가 또 가네요.      0\n","1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n","2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n","3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n","4                      PPL 심하네                눈살이 찌푸려지죠.      0\n","...                        ...                       ...    ...\n","11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n","11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n","11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n","11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n","11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n","\n","[11823 rows x 3 columns]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":[""],"metadata":{"id":"oqVZpMQsdoGD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 2. 데이터 전처리하기\n","\n","영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다."],"metadata":{"id":"_D-CbfJgDFgd"}},{"cell_type":"markdown","source":["- **정규 표현식(Regular Expression)** 을 사용\n","- **구두점(punctuation)** 을 제거하여 단어를 **토크나이징(tokenizing)** 하는 일에 방해가 되지 않도록 정제하는 것을 목표"],"metadata":{"id":"pbhJYayhwgiC"}},{"cell_type":"markdown","source":["🎼 예제에서는 영문으로 된 데이터셋을 사용했어서, 우수노드에 있는 코드를 참고했다. 하지만 그 코드에서도 숫자는 제외되어있어서 프린트문으로 출력해보니 이렇게 출력되었다.\n","```\n","print('전처리 후의 첫번째 질문 샘플: {}'.format(questions[0]))\n","전처리 후의 첫번째 질문 샘플: 시 땡 !\n","```\n","`12시`라는 시간의 의미가 있을 것 같아서 정규표현식에 **숫자**도 출력될 수 있도록 추가했다."],"metadata":{"id":"_E1K2ah98p6e"}},{"cell_type":"code","source":["# 전처리 함수\n","def preprocess_sentence(sentence):\n","    # 영어 포함 시 소문자 변환, 양쪽 공백 제거\n","    sentence = sentence.lower().strip()\n","    \n","    # 단어와 구두점 사이 공백 추가\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n","    \n","    # 한글, 알파벳, .,?!을 제외한 문자 공백으로 대체\n","    sentence = re.sub(r\"[^가-힣1-9a-zA-Z?.!,]+\", \" \", sentence)\n","    sentence = sentence.strip()\n","    \n","    return sentence"],"metadata":{"id":"AiC-Ow3qfHhC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 데이터를 로드하는 동시에 전처리 함수를 호출하여 질문과 답변의 쌍을 전처리"],"metadata":{"id":"CMgiEYhUgJd7"}},{"cell_type":"code","source":["# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n","def load_conversations():\n","    \n","     inputs, outputs = [], []\n","     with open(dataset_path, errors='ignore') as file:\n","          lines = csv.reader(file)\n","          next(lines)\n","          \n","          for line in lines:\n","                # 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용.\n","                inputs.append(preprocess_sentence(line[0]))\n","                outputs.append(preprocess_sentence(line[1]))\n","\n","     return inputs, outputs\n","\n","print(\"슝=3\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z2xUbB38fHUW","executionInfo":{"status":"ok","timestamp":1645565762053,"user_tz":-540,"elapsed":100,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"8e90d037-0d56-43ae-85b2-c813296cef6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"code","source":["# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n","questions, answers = load_conversations()\n","print('전체 샘플 수 :', len(questions))\n","print('전체 샘플 수 :', len(answers))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCubF-a4fHLU","executionInfo":{"status":"ok","timestamp":1645565762057,"user_tz":-540,"elapsed":76,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"a1694c06-a8b1-48ef-f34f-f6c29131789c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["전체 샘플 수 : 11823\n","전체 샘플 수 : 11823\n"]}]},{"cell_type":"markdown","source":["- 질문과 답변은 병렬적으로 구성되는 데이터셋이므로 <u>두 샘플 수는 정확하게 일치</u>해야 한다."],"metadata":{"id":"BmWdZ2HR5w3F"}},{"cell_type":"code","source":["print('전처리 후의 첫번째 질문 샘플: {}'.format(questions[0]))\n","print('전처리 후의 첫번째 답변 샘플: {}'.format(answers[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JK5laCp-w0GZ","executionInfo":{"status":"ok","timestamp":1645565762061,"user_tz":-540,"elapsed":70,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"d261aa16-4030-46b4-cbf4-0afc58f1975b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["전처리 후의 첫번째 질문 샘플: 12시 땡 !\n","전처리 후의 첫번째 답변 샘플: 하루가 또 가네요 .\n"]}]},{"cell_type":"markdown","source":["- `?`나 `.`과 같은 구두점들이 단어들과 분리되어 단어와 구두점 사이에는 공백이 추가된 것을 확인\n","- **숫자**도 잘 출력되고 있다. \n","- 단어를 토크나이징 하는 과정에서 <u>구두점과 붙어있던 단어들을 하나의 단어로 인식하는 것을 방지</u>할 수 있다."],"metadata":{"id":"tMMSBPZ8w_jR"}},{"cell_type":"code","source":[""],"metadata":{"id":"CcV81LQifHB1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"YGGaWQZWDKwb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 3. SubwordTextEncoder 사용하기\n","\n","한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 `SubwordTextEncoder`를 그대로 사용해보세요."],"metadata":{"id":"YzV_3v68DLM5"}},{"cell_type":"markdown","source":["## 병렬 데이터 전처리하기\n","\n","<br>\n","\n","1. TensorFlow Datasets **SubwordTextEncoder**를 토크나이저로 사용한다.  단어보다 더 작은 단위인 Subword를 기준으로 토크나이징하고,  각 토큰을 고유한 **정수로 인코딩**한다.\n","2. 각 문장을 토큰화하고 각 문장의 시작과 끝을 나타내는 `START_TOKEN` 및 `END_TOKEN`을 추가한다.\n","3. **MAX_LENGTH**를 정하기 위해서, 전체 문장의 길이를 파악한 뒤 적정한 수를 구한다.\n","4. 최대 길이 **MAX_LENGTH**인 ***10***을 넘는 문장들은 필터링한다.\n","5. MAX_LENGTH보다 길이가 짧은 문장들은 ***10***에 맞도록 **패딩** 한다."],"metadata":{"id":"VVzEENBKxHEp"}},{"cell_type":"markdown","source":["**1. 단어장(Vocabulary) 만들기**\n","\n","- 우선 각 단어에 고유한 정수 인덱스를 부여하기 위해서 단어장(Vocabulary)을 만들어본다.\n","- 질문과 답변 데이터셋을 모두 사용해서 단어장을 만든다.\n","\n"],"metadata":{"id":"aBzPa29txj-p"}},{"cell_type":"code","source":["import tensorflow_datasets as tfds\n","print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n","\n","# 질문과 답변 데이터셋에 대해서 Vocabulary 생성. (Tensorflow 2.3.0 이상) (클라우드는 2.4 입니다)\n","tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n","print(\"슝=3 \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TecLyg6zxjgd","executionInfo":{"status":"ok","timestamp":1645565785033,"user_tz":-540,"elapsed":23022,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"74ded05d-38cd-4875-b58c-c065ac879b28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\n","슝=3 \n"]}]},{"cell_type":"code","source":["import tensorflow_datasets as tfds\n","print(\"슝=3 \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-YIExCfkxmhU","executionInfo":{"status":"ok","timestamp":1645565785036,"user_tz":-540,"elapsed":163,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"3b4697f5-c694-46e5-a820-e9a366e259f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3 \n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"lgs3y0RfCcHM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 디코더의 문장 생성 과정에서 사용할 **'시작 토큰'**과 **'종료 토큰'**에 대해서도 임의로 단어장에 추가하여서 정수를 부여한다.\n","- 이미 생성된 단어장의 번호와 겹치지 않도록 각각 단어장의 크기와 그보다 1이 큰 수를 번호로 부여"],"metadata":{"id":"LYYtjwfZxt8l"}},{"cell_type":"code","source":["# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n","START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n","print(\"슝=3\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObBj-NcJxr9h","executionInfo":{"status":"ok","timestamp":1645565785040,"user_tz":-540,"elapsed":142,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"2fffd837-332f-4b11-fcc8-b1a509b39f8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"markdown","source":["시작 토큰과 종료 토큰에 부여된 정수를 출력해봅시다."],"metadata":{"id":"G9sGb1ZhxyiN"}},{"cell_type":"code","source":["print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n","print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMx3f4SGxwwy","executionInfo":{"status":"ok","timestamp":1645565785041,"user_tz":-540,"elapsed":132,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"f8d0878e-551e-434c-d054-3cb4f870fec9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["START_TOKEN의 번호 : [8158]\n","END_TOKEN의 번호 : [8159]\n"]}]},{"cell_type":"markdown","source":["- 단어장의 크기 : **8,158** (0\\~8,157)\n","- `START_TOKEN`와 `END_TOKEN`을 추가하였으니 **8,160**개가 되었다."],"metadata":{"id":"gNNA4jhXx2RC"}},{"cell_type":"code","source":["# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n","VOCAB_SIZE = tokenizer.vocab_size + 2\n","print(VOCAB_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ezVFeYTx0qh","executionInfo":{"status":"ok","timestamp":1645565785043,"user_tz":-540,"elapsed":124,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"0872927c-2ecc-4b05-e4f4-98bac21bb6ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8160\n"]}]},{"cell_type":"markdown","source":["- 단어장 갯수 다시 확인"],"metadata":{"id":"yuYssWIpEEzW"}},{"cell_type":"code","source":[""],"metadata":{"id":"34_B2xcwALxx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2. 각 단어를 고유한 정수로 인코딩(Integer encoding) & 패딩(Padding)**\n","\n","<br>\n","\n","- 위에서 `tensorflow_datasets`의 `SubwordTextEncoder`를 사용해서 tokenizer를 정의하고 Vocabulary를 만들었다.\n","- `tokenizer.encode()`로 각 <u>단어를 정수</u>로 변환할 수 있다.\n","-  `tokenizer.decode()`를 통해 정수 시퀀스를 <u>단어 시퀀스</u>로 변환할 수 있다.\n","\n","<br>\n","\n","- 22번째 샘플을 `tokenizer.encode()`의 입력으로 사용해서 변환 결과를 확인해 보자"],"metadata":{"id":"iKXymb2Hx8Vb"}},{"cell_type":"code","source":["# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n","# 각 토큰을 고유한 정수로 변환\n","print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n","print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vOVImHBzyI7D","executionInfo":{"status":"ok","timestamp":1645565785047,"user_tz":-540,"elapsed":116,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"1f1c1a0a-4c56-4441-c86b-5683fd68c488"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["정수 인코딩 후의 21번째 질문 샘플: [5757, 610, 2489, 4164]\n","정수 인코딩 후의 21번째 답변 샘플: [2356, 7501, 7, 6266, 97, 1]\n"]}]},{"cell_type":"markdown","source":["- 질문과 답변 셋에 대해서 전부 **정수 인코딩**을 수행한다.\n","- 이와 동시에 문장의 최대 길이를 정하고, 해당 길이로 **패딩(padding)**을 해 준다."],"metadata":{"id":"ZDLGaH9Y0xwZ"}},{"cell_type":"code","source":[""],"metadata":{"id":"X5paOvBRGIdN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 먼저, 패딩을 하기 전에, **MAX_LENGTH**를 정하기 위해 전체적인 문장의 길이를 파악해 본다."],"metadata":{"id":"MP6a5fbkGI0L"}},{"cell_type":"code","source":["# 각 문장들을 꺼내서 몇개의 단어(토큰)으로 이루어져 있는지 확인\n","questions_len = [len(s.split()) for s in questions]\n","answers_len = [len(s.split()) for s in answers]\n","\n","# 최대, 평균 길이\n","print(f'questions 최대 길이 : {np.max(questions_len)}')\n","print(f'questions 평균 길이 : {np.mean(questions_len)}')\n","print(f'answers 최대 길이 : {np.max(answers_len)}')\n","print(f'answers 평균 길이 : {np.mean(answers_len)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-t36SW-xGR2L","executionInfo":{"status":"ok","timestamp":1645565785050,"user_tz":-540,"elapsed":107,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"185b84dd-477c-4041-b183-7e90f81272ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["questions 최대 길이 : 16\n","questions 평균 길이 : 3.9406242070540474\n","answers 최대 길이 : 24\n","answers 평균 길이 : 4.716907722236319\n"]}]},{"cell_type":"markdown","source":["- 문장 길이별 분포를 확인"],"metadata":{"id":"elj38QkSHwj9"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10,5))\n","plt.subplot(1,2,1)\n","plt.title('questions')\n","plt.hist(questions_len, bins = 40, color='steelblue')\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","\n","plt.subplot(1,2,2)\n","plt.title('answers')\n","plt.hist(answers_len, bins = 40, color='steelblue')\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"a3c2EioOGRfD","executionInfo":{"status":"ok","timestamp":1645565785616,"user_tz":-540,"elapsed":648,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"455b946c-572c-495b-ee91-653f9de6d4cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'number of samples')"]},"metadata":{},"execution_count":15},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x360 with 2 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["- `Question`, `Answer` : 토큰 갯수가 10개가 넘어가면 문장의 갯수가 많이 줄어든다. \n","👉 <u>**MAX_LENGTH** 를 10으로 정해본다!</u>"],"metadata":{"id":"vcXc9izDgNKK"}},{"cell_type":"code","source":["# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n","MAX_LENGTH = 10\n","print(MAX_LENGTH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_bJ11OuyJuN","executionInfo":{"status":"ok","timestamp":1645565785623,"user_tz":-540,"elapsed":50,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"6e7164ec-69de-456d-a98a-af3c76939990"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10\n"]}]},{"cell_type":"code","source":["# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n","def tokenize_and_filter(inputs, outputs):\n","  tokenized_inputs, tokenized_outputs = [], []\n","  \n","  for (sentence1, sentence2) in zip(inputs, outputs):\n","    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n","    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n","    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n","\n","    # 최대 길이 10 이하인 경우에만 데이터셋으로 허용\n","    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n","      tokenized_inputs.append(sentence1)\n","      tokenized_outputs.append(sentence2)\n","  \n","  # 최대 길이 10으로 모든 데이터셋을 패딩\n","  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n","  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n","  \n","  return tokenized_inputs, tokenized_outputs\n","print(\"슝=3\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n_Aeak7H06O3","executionInfo":{"status":"ok","timestamp":1645565785625,"user_tz":-540,"elapsed":35,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"96243d88-e9be-4e66-8e89-423c42789fb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"markdown","metadata":{"id":"eysH0BLc1BER"},"source":["- 정수 인코딩 과정을 수행하면서 샘플의 길이가 10을 넘는 경우는 샘플들을 필터링하였으므로 일부 샘플이 제외되었다.\n","- 단어장의 크기와 샘플의 개수를 확인 해 보자."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":482,"status":"ok","timestamp":1645565786082,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"G-lBb6NK0_ZU","outputId":"4e4bb85c-8d76-4fbf-df7c-2bdda97bbacb"},"outputs":[{"output_type":"stream","name":"stdout","text":["단어장의 크기 : 8160\n","필터링 후의 질문 샘플 개수: 9107\n","필터링 후의 답변 샘플 개수: 9107\n","약 0.77 정도의 샘플이 남아있다.\n"]}],"source":["questions, answers = tokenize_and_filter(questions, answers)\n","print('단어장의 크기 :',(VOCAB_SIZE))\n","print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n","print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))\n","print('약 %.2f 정도의 샘플이 남아있다.' % (len(questions)/11823))"]},{"cell_type":"markdown","metadata":{"id":"Xj24Nuwr1HkZ"},"source":["**3. 교사 강요(Teacher Forcing) 사용하기**\n","\n","<br>\n","\n","- **tf.data.Dataset API**는 훈련 프로세스의 속도가 빨라지도록 입력 파이프라인을 구축하는 API이다.\n","- 질문과 답변의 쌍을 `tf.data.Dataset`의 입력으로 넣어주는 작업을 해주어야 한다.\n","- 이때, 디코더의 입력과 실제값(레이블)을 정의해 주기 위해서는 **교사 강요(Teacher Forcing)**를 사용한다.\n","\n","![](https://wikidocs.net/images/page/46496/rnnlm2_final_final.PNG)\n","- 예측한 값을 넣는 것이 아니라, 실제 값을 다음 timestep에 넣고, 예측값과의 `cross-entropy`를 구한다.\n","\n","<br>\n","\n","- 교사 강요를 하지 않은 경우, <u>잘못된 예측이 다음 시점(time step)의 입력으로 들어가면서 연쇄적으로 예측 정확도에 영향을 미쳐서 학습 시간이 느려지게 되므로</u> 교사 강요를 사용하여 RNN을 좀 더 빠르고 효과적으로 훈련시킬 수 있게 된다.\n","\n","<br>\n","\n","- **자기회귀 모델(auto-regressive model, AR)** : 이전 자신의 출력이 현재 자신의 상태를 결정하는 모델. *RNN 언어 모델(RNN language model, RNNLM)*은 대표적인 자기 회귀 모델의 예이며, *트랜스포머의 디코더* 또한 자기회귀 모델이다.\n","\n","<br>\n","\n","- 트랜스포머 디코더에서도 **교사 강요(Teacher Forcing)** 를 적용한다.\n"]},{"cell_type":"markdown","metadata":{"id":"C9YJe7-D6M94"},"source":["- 질문과 답변의 쌍을 **tf.data.Dataset API**의 입력으로 사용하여 파이프라인을 구성한다.\n","- 이때, 교사 강요를 위해서 `answers[:, :-1]`를 디코더의 입력값, `answers[:, 1:]`를 디코더의 레이블로 사용한다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":535,"status":"ok","timestamp":1645568910716,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"FmA5MpEo2ymQ","outputId":"5ce7d051-4413-4703-8c0f-20288d28a861"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["BATCH_SIZE = 16\n","BUFFER_SIZE = len(questions)    # buffer size를 샘플의 갯수로 조정한다.\n","\n","# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n","# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n","dataset = tf.data.Dataset.from_tensor_slices((\n","    { 'inputs': questions,\n","     'dec_inputs': answers[:, :-1]},\n","    {'outputs': answers[:, 1:]}))\n","\n","dataset = dataset.cache()\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE)\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","print(\"슝=3\")"]},{"cell_type":"code","source":[""],"metadata":{"id":"aB5qTOWGDPKE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 4. 모델 구성하기\n","\n","위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."],"metadata":{"id":"dYlBcOnEDQjs"}},{"cell_type":"markdown","source":["**\n","\n"," - 트랜스포머는 입력을 받을 때, 문장에 있는 단어들을 1개씩 순차적으로 받는 것이 아니라, 문장에 있는 모든 단어를 한꺼번에 입력으로 받기 때문에, <u>같은 단어라도 그 단어가 문장의 몇 번째 어순으로 입력되었는지를 모델에 추가로 알려 주는 작업이 필요</u>하다.\n"," - 단어의 임베딩 벡터에다가 **위치 정보를 가진 벡터(Positional Encoding)** 값을 더하는 방법을 사용\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_5_kH52kQN.png)\n","\n","<br>\n","\n","$$\\begin{array}{c}\n","P E_{(p o s, 2 i)}=\\sin \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right) \\\\\n","P E_{(p o s, 2 i+1)}=\\cos \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right)\n","\\end{array}$$\n","\n","<br> \n","\n","- 포지셔널 인코딩의 벡터값은 위의 수식에 의해서 정해진다.\n","- 사인 함수와 코사인 함수의 그래프를 상기해보면 요동치는 값의 형태를 생각해 볼 수 있는데, 트랜스포머는 <u>사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해줌으로써 단어의 순서 정보를 더한다.</u>\n","\n","<br>\n","\n","- 임베딩 벡터와 포지셔널 인코딩의 덧셈은 사실 <u>임베딩 벡터가 모여 만들어진 ***문장 벡터 행렬***과 ***포지셔널 인코딩 행렬***의 덧셈 연산을 통해 이루어진다</u>\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_7_3Rneu0P.png)\n","\n","- **$d_{model}$**은 <u>임베딩 벡터의 차원</u>을 의미\n","- **$pos$**는 입력 문장에서의 <u>임베딩 벡터의 위치</u>를 나타낸다.\n","- **$i$**는 임베딩 벡터 내의 <u>차원의 인덱스</u>를 의미"],"metadata":{"id":"MHtujI6CE8w0"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1645565787316,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"KLzUpXyKk2RT","outputId":"4a51aba0-e583-4ea7-dc20-9606c4d66a18"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["# 포지셔널 인코딩 레이어\n","class PositionalEncoding(tf.keras.layers.Layer):\n","\n","  def __init__(self, position, d_model):\n","    super(PositionalEncoding, self).__init__()\n","    self.pos_encoding = self.positional_encoding(position, d_model)\n","\n","  def get_angles(self, position, i, d_model):\n","    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n","    return position * angles\n","\n","  def positional_encoding(self, position, d_model):\n","    # 각도 배열 생성\n","    angle_rads = self.get_angles(\n","        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n","        d_model=d_model)\n","\n","    # 배열의 짝수 인덱스에는 sin 함수 적용\n","    sines = tf.math.sin(angle_rads[:, 0::2])\n","    # 배열의 홀수 인덱스에는 cosine 함수 적용\n","    cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","    # sin과 cosine이 교차되도록 재배열\n","    pos_encoding = tf.stack([sines, cosines], axis=0)\n","    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n","    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n","\n","    pos_encoding = pos_encoding[tf.newaxis, ...]\n","    return tf.cast(pos_encoding, tf.float32)\n","\n","  def call(self, inputs):\n","    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n","\n","print(\"슝=3\")"]},{"cell_type":"markdown","source":["**스케일드 닷 프로덕트 어텐션**\n","\n","<br>\n","\n","$$\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$$\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_16_neA52rZ.png)\n","\n","- 내적(dot product)을 통해 단어 벡터 간 유사도를 구한 후에, 특정 값을 분모로 나눠주는 방식\n","\n","\n","<br>\n","\n","---\n","\n","<br>\n","\n","- 어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 **유사도**를 각각 구한다.\n","- 이 유사도를 키와 맵핑되어있는 각각의 '값(Value)'에 반영\n","- 유사도가 반영된 '값(Value)'을 모두 더해서 뭉쳐주면 이를 최종 결과인 **어텐션 값(Attention Value)**이라고 한다.\n","\n","<br>\n","\n","위 정의와 아래 내용 세 가지만 기억하면 수식을 그림으로 정리할 수 있다.\n","\n","<br>\n","\n","1. $Q, K, V$는 단어 벡터를 행으로 하는 문장 행렬이다.\n","2. 벡터의 **내적(dot product)** 은 벡터의 **유사도**를 의미한다.\n","3. 특정 값을 분모로 사용하는 것은 값의 크기를 조절하는 스케일링(Scaling)을 위함이다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_15_pUfIgKn.png)\n","\n","\n","***초록색 행렬***이 의미하는 값 : \n","\n","- 'am' 행과 'student' 열의 값\n","-  $Q$ 행렬에 있던 'am' 벡터와 $K$ 행렬에 있던 'student 벡터'의 **내적값**을 의미한다. (벡터의 내적 = 유사도)\n","- <u>각 단어 벡터의 유사도가 모두 기록된</u> ***유사도 행렬***이 된다.\n"],"metadata":{"id":"YsZKR-8CHcxC"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1645565787321,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"nLhRBPIY7zmu","outputId":"af4eb12a-4980-489d-e0f7-06f55c16cb5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["# 스케일드 닷 프로덕트 어텐션 함수\n","def scaled_dot_product_attention(query, key, value, mask):\n","  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n","  matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","  # 가중치를 정규화\n","  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","  logits = matmul_qk / tf.math.sqrt(depth)\n","\n","  # 패딩에 마스크 추가\n","  if mask is not None:\n","    logits += (mask * -1e9)\n","\n","  # softmax적용\n","  attention_weights = tf.nn.softmax(logits, axis=-1)\n","\n","  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n","  output = tf.matmul(attention_weights, value)\n","  return output\n","\n","print(\"슝=3\")"]},{"cell_type":"markdown","source":["**멀티 헤드 어텐션**\n","\n","<br>\n","\n","![](https://media.vlpt.us/images/cha-suyeon/post/2e70e601-e268-4b55-a8c9-11b3ff145d92/image.png)\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_19_FwmaA3q.png)\n","\n","- 위 그림은 `num_heads`의 값이 8일 때, 병렬로 수행되는 어텐션이 <u>서로 다른 셀프 어텐션 결과를 얻을 수 있다는 것을 보여준다.</u>\n","- 다시 말해 8개의 머리는 <u>각각 다른 관점에서 어텐션을 수행하므로 한 번의 어텐션만 수행했다면 놓칠 수도 있던 정보를 캐치할 수 있다.</u> \n","- 예를 들어 위 그림에서라면 `it_`이라는 토큰이 `animal_`과 유사하다고 보는 관점과 `street_`과 유사하다고 보는 관점이 한꺼번에 모두 표현 가능하다는 뜻"],"metadata":{"id":"2bHS_YmJJWFS"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":129,"status":"ok","timestamp":1645565787323,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"gCBIoHoP8XsY","outputId":"f7d3cac8-6506-47e5-ebf2-6abdd1a7f918"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n","    super(MultiHeadAttention, self).__init__(name=name)\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    self.depth = d_model // self.num_heads\n","\n","    self.query_dense = tf.keras.layers.Dense(units=d_model)\n","    self.key_dense = tf.keras.layers.Dense(units=d_model)\n","    self.value_dense = tf.keras.layers.Dense(units=d_model)\n","\n","    self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","  def split_heads(self, inputs, batch_size):\n","    inputs = tf.reshape(\n","        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","  def call(self, inputs):\n","    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n","        'value'], inputs['mask']\n","    batch_size = tf.shape(query)[0]\n","\n","    # Q, K, V에 각각 Dense를 적용합니다\n","    query = self.query_dense(query)\n","    key = self.key_dense(key)\n","    value = self.value_dense(value)\n","\n","    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n","    query = self.split_heads(query, batch_size)\n","    key = self.split_heads(key, batch_size)\n","    value = self.split_heads(value, batch_size)\n","\n","    # 스케일드 닷 프로덕트 어텐션 함수\n","    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n","\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n","    concat_attention = tf.reshape(scaled_attention,\n","                                  (batch_size, -1, self.d_model))\n","\n","    # 최종 결과에도 Dense를 한 번 더 적용합니다\n","    outputs = self.dense(concat_attention)\n","\n","    return outputs\n","print(\"슝=3\")"]},{"cell_type":"markdown","source":["**패딩 마스킹**\n","\n","<br>\n","\n","- 케라스의 `pad_sequences()`를 사용하여 패딩\n","- 이렇게 주어진 숫자 <u>0은 실제 의미가 있는 단어가 아니므로 실제 어텐션 등과 같은 연산에서는 제외</u>한다.\n","- **패딩 마스킹**은 이를 위해 숫자 0인 위치를 체크"],"metadata":{"id":"qLi8bvycKc12"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1645565787325,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"o63qzObcBqYu","outputId":"1b14afc2-2b39-4b5e-a6c7-341eccf407ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["def create_padding_mask(x):\n","  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","  # (batch_size, 1, 1, sequence length)\n","  return mask[:, tf.newaxis, tf.newaxis, :]\n","print(\"슝=3\")"]},{"cell_type":"markdown","source":["**룩 어헤드 마스킹(Look-ahead masking, 다음 단어 가리기)**\n","\n","<br>\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/_.max-800x600.png)"],"metadata":{"id":"5GPFLu0CK6Sa"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110,"status":"ok","timestamp":1645565787327,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"8aj3I6czCZ06","outputId":"01e53e48-abd3-497a-a874-079fd8ab426a"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["def create_look_ahead_mask(x):\n","  seq_len = tf.shape(x)[1]\n","  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","  padding_mask = create_padding_mask(x)\n","  return tf.maximum(look_ahead_mask, padding_mask)\n","print(\"슝=3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101,"status":"ok","timestamp":1645565787329,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"qi9Lr6XfJ6pk","outputId":"4a402ad7-a2c6-42c2-8207-114c5eaf6f0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["# 인코더 하나의 레이어를 함수로 구현.\n","# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n","def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","\n","  # 패딩 마스크 사용\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n","  attention = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention\")({\n","          'query': inputs,\n","          'key': inputs,\n","          'value': inputs,\n","          'mask': padding_mask\n","      })\n","\n","  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n","  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","  attention = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(inputs + attention)\n","\n","  # 두 번째 서브 레이어 : 2개의 완전연결층\n","  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention + outputs)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","print(\"슝=3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90,"status":"ok","timestamp":1645565787331,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"waopVY57KN9R","outputId":"47e9cf26-8a80-4955-c8ab-66b4ef1013e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["def encoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name=\"encoder\"):\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","  # 패딩 마스크 사용\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # 임베딩 레이어\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","\n","  # 포지셔널 인코딩\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  # num_layers만큼 쌓아올린 인코더의 층.\n","  for i in range(num_layers):\n","    outputs = encoder_layer(\n","        units=units,\n","        d_model=d_model,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","        name=\"encoder_layer_{}\".format(i),\n","    )([outputs, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","print(\"슝=3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82,"status":"ok","timestamp":1645565787333,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"0tQEGjUQKwvE","outputId":"38b4baed-2d39-48ba-daf3-3024661be354"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["# 디코더 하나의 레이어를 함수로 구현.\n","# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n","def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name=\"look_ahead_mask\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n","  attention1 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_1\")(inputs={\n","          'query': inputs,\n","          'key': inputs,\n","          'value': inputs,\n","          'mask': look_ahead_mask\n","      })\n","\n","  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n","  attention1 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention1 + inputs)\n","\n","  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n","  attention2 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_2\")(inputs={\n","          'query': attention1,\n","          'key': enc_outputs,\n","          'value': enc_outputs,\n","          'mask': padding_mask\n","      })\n","\n","  # 마스크드 멀티 헤드 어텐션의 결과는\n","  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n","  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n","  attention2 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention2 + attention1)\n","\n","  # 세 번째 서브 레이어 : 2개의 완전연결층\n","  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(outputs + attention2)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","print(\"슝=3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72,"status":"ok","timestamp":1645565787334,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"kfYpY0SiBuCU","outputId":"48524964-7228-4015-8613-2620c34624b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["def decoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name='decoder'):\n","  inputs = tf.keras.Input(shape=(None,), name='inputs')\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name='look_ahead_mask')\n","\n","  # 패딩 마스크\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","  \n","  # 임베딩 레이어\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","\n","  # 포지셔널 인코딩\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","  # Dropout이라는 훈련을 돕는 테크닉을 수행\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  for i in range(num_layers):\n","    outputs = decoder_layer(\n","        units=units,\n","        d_model=d_model,\n","        num_heads=num_heads,\n","        dropout=dropout,\n","        name='decoder_layer_{}'.format(i),\n","    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","print(\"슝=3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65,"status":"ok","timestamp":1645565787337,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"qS_LBru_6bou","outputId":"ae118151-d0c1-4756-fb97-6165c0e746dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["def transformer(vocab_size,\n","                num_layers,\n","                units,\n","                d_model,\n","                num_heads,\n","                dropout,\n","                name=\"transformer\"):\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n","\n","  # 인코더에서 패딩을 위한 마스크\n","  enc_padding_mask = tf.keras.layers.Lambda(\n","       create_padding_mask, output_shape=(1, 1, None),\n","      name='enc_padding_mask')(inputs)\n","\n","  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n","  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n","  look_ahead_mask = tf.keras.layers.Lambda(\n","      create_look_ahead_mask,\n","      output_shape=(1, None, None),\n","      name='look_ahead_mask')(dec_inputs)\n","\n","  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n","  # 디코더에서 패딩을 위한 마스크\n","  dec_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='dec_padding_mask')(inputs)\n","\n","  # 인코더\n","  enc_outputs = encoder(\n","      vocab_size=vocab_size,\n","      num_layers=num_layers,\n","      units=units,\n","      d_model=d_model,\n","      num_heads=num_heads,\n","      dropout=dropout,\n","  )(inputs=[inputs, enc_padding_mask])\n","\n","  # 디코더\n","  dec_outputs = decoder(\n","      vocab_size=vocab_size,\n","      num_layers=num_layers,\n","      units=units,\n","      d_model=d_model,\n","      num_heads=num_heads,\n","      dropout=dropout,\n","  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n","\n","  # 완전연결층\n","  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n","\n","  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n","print(\"슝=3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MGzZgBXc6dOQ"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"j1XXL_jS6e9c"},"source":["**1. 모델 생성**\n","\n","- `num_layers`, `d-Model`, `units`는 전부 사용자가 정할 수 있는 하이퍼파라미터 값이다.\n","- 논문에서 `num_layers`는 6, `d-Model`은 512였지만, 빠르고 원활한 훈련을 위해 실습에서는 각 하이퍼파라미터를 논문에서보다는 작은 값을 사용했다.\n","- 지금은 논문과 같이 적용해 본다. (`drop out`은 0.5로 조정해본다.) ❗ 0.3에서 갑자기 accracy가 떨어지는 현상이 생겼었음"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9391,"status":"ok","timestamp":1645565796674,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"o979JabU7qY6","outputId":"95b02311-6b0e-48bf-ffec-be29a100f0eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," inputs (InputLayer)            [(None, None)]       0           []                               \n","                                                                                                  \n"," dec_inputs (InputLayer)        [(None, None)]       0           []                               \n","                                                                                                  \n"," enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n","                                                                                                  \n"," encoder (Functional)           (None, None, 64)     1020288     ['inputs[0][0]',                 \n","                                                                  'enc_padding_mask[0][0]']       \n","                                                                                                  \n"," look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n","                                e)                                                                \n","                                                                                                  \n"," dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n","                                                                                                  \n"," decoder (Functional)           (None, None, 64)     1120896     ['dec_inputs[0][0]',             \n","                                                                  'encoder[0][0]',                \n","                                                                  'look_ahead_mask[0][0]',        \n","                                                                  'dec_padding_mask[0][0]']       \n","                                                                                                  \n"," outputs (Dense)                (None, None, 8160)   530400      ['decoder[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 2,671,584\n","Trainable params: 2,671,584\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["tf.keras.backend.clear_session()\n","\n","# 하이퍼파라미터\n","NUM_LAYERS =  6 # 인코더와 디코더의 층의 개수, 인코더 함수 안애서 지정된 수 만큼의 레이어를 쌓아준다.\n","D_MODEL = 64  # 인코더와 디코더 내부의 입, 출력의 고정 차원\n","NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n","UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n","DROPOUT = 0.5 # 드롭아웃의 비율\n","\n","model = transformer(\n","    vocab_size=VOCAB_SIZE,\n","    num_layers=NUM_LAYERS,\n","    units=UNITS,\n","    d_model=D_MODEL,\n","    num_heads=NUM_HEADS,\n","    dropout=DROPOUT)\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"Mn6ecmjr7tfk"},"source":["**2. 손실 함수(Loss function)**\n","\n","- 레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용해야 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1645565796683,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"47SG8k047vzA","outputId":"1455a105-0271-48af-fbfb-6d534c8b2483"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["def loss_function(y_true, y_pred):\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","  \n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","      from_logits=True, reduction='none')(y_true, y_pred)\n","\n","  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","  loss = tf.multiply(loss, mask)\n","\n","  return tf.reduce_mean(loss)\n","print(\"슝=3\")"]},{"cell_type":"markdown","metadata":{"id":"HCvDOeCC7x5z"},"source":["**3. 커스텀 된 학습률(Learning rate)**\n","\n","<br>\n","\n","- 딥러닝 모델학습 시 `learning rate`는 매우 중요한 하이퍼파라미터이다.\n","-  **커스텀 학습률 스케줄링(Custom Learning rate Scheduling)** : 최근 많이 쓰이고 있는 <u>모델학습 초기에 learning rate를 급격히 높였다가, 이후 train step이 진행됨에 따라 서서히 낮추어 가면서 안정적으로 수렴하게 하는 고급 기법</u>\n","\n","<br>\n","\n","- 논문에 나온 공식을 참고하여 커스텀 학습률 스케줄러를 통한 **아담 옵티마이저**를 사용\n","- 논문에 나온 공식은 다음과 같다.\n","\n","<br>\n","\n","$$\\text { lrate }=d_{\\text {model }}^{-0.5} \\cdot \\min \\left(\\text { step_num }^{-0.5}, \\text { step_num } \\cdot \\text { warmup_steps }^{-1.5}\\right)$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110,"status":"ok","timestamp":1645565796687,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"kkoqDETm8L5R","outputId":"073b4722-c352-49cc-c25f-20df83f840b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps**-1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","print(\"슝=3\")"]},{"cell_type":"markdown","metadata":{"id":"zv4gyZ4t8fOK"},"source":["- 방금 정의한 커스텀 학습률 스케줄링 계획을 시각화\n","- 위에 언급한 수식은 $step_num^{−0.5}$에 비례하는 부분과 $step_num$에 비례하는 부분 중 작은 쪽을 택하도록 되어 있기 때문에, 학습 초기에는 learning_rate가 $step_num$에 비례해서 증가하다가 이후로는 감소하는 것을 확인할 수 있다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"elapsed":101,"status":"ok","timestamp":1645565796690,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"pQDidbJy8cmt","outputId":"aeffef91-3884-4bb7-ef29-53f75bb5b5d7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'Train Step')"]},"metadata":{},"execution_count":33},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["sample_learning_rate = CustomSchedule(d_model=64)\n","\n","plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n","plt.ylabel(\"Learning Rate\")\n","plt.xlabel(\"Train Step\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gWJtvQE8yT-"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"50DJMD7l812A"},"source":["**4. 모델 컴파일**\n","\n","손실 함수와 커스텀 된 학습률(learning rate)을 사용하여 모델을 컴파일합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92,"status":"ok","timestamp":1645565796697,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"id":"cNfCK3C2828R","outputId":"1b6a05eb-b30c-4567-82fc-0dd2d6736328"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","def accuracy(y_true, y_pred):\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n","\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n","print(\"슝=3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VzZsyRpJ84RZ"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"VONhh3fw8-nm"},"source":["**5. 훈련하기**\n","\n","- 총 200 에포크를 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pECCHAoL9BCI","executionInfo":{"status":"ok","timestamp":1645568818538,"user_tz":-540,"elapsed":2392261,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"b7bf0aec-2a5b-45fc-d223-6df619afb5a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","143/143 [==============================] - 35s 78ms/step - loss: 5.8796 - accuracy: 0.0663\n","Epoch 2/200\n","143/143 [==============================] - 11s 78ms/step - loss: 5.4638 - accuracy: 0.1111\n","Epoch 3/200\n","143/143 [==============================] - 11s 78ms/step - loss: 4.8016 - accuracy: 0.1111\n","Epoch 4/200\n","143/143 [==============================] - 11s 78ms/step - loss: 4.1790 - accuracy: 0.1131\n","Epoch 5/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.8644 - accuracy: 0.1834\n","Epoch 6/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.6460 - accuracy: 0.2135\n","Epoch 7/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.5017 - accuracy: 0.2167\n","Epoch 8/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.3959 - accuracy: 0.2185\n","Epoch 9/200\n","143/143 [==============================] - 11s 79ms/step - loss: 3.3256 - accuracy: 0.2211\n","Epoch 10/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.2797 - accuracy: 0.2234\n","Epoch 11/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.2362 - accuracy: 0.2250\n","Epoch 12/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.1951 - accuracy: 0.2263\n","Epoch 13/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.1564 - accuracy: 0.2279\n","Epoch 14/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.1141 - accuracy: 0.2308\n","Epoch 15/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.0809 - accuracy: 0.2309\n","Epoch 16/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.0424 - accuracy: 0.2326\n","Epoch 17/200\n","143/143 [==============================] - 11s 78ms/step - loss: 3.0033 - accuracy: 0.2349\n","Epoch 18/200\n","143/143 [==============================] - 12s 86ms/step - loss: 2.9636 - accuracy: 0.2368\n","Epoch 19/200\n","143/143 [==============================] - 11s 79ms/step - loss: 2.9171 - accuracy: 0.2394\n","Epoch 20/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.8706 - accuracy: 0.2415\n","Epoch 21/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.8267 - accuracy: 0.2427\n","Epoch 22/200\n","143/143 [==============================] - 11s 79ms/step - loss: 2.7805 - accuracy: 0.2451\n","Epoch 23/200\n","143/143 [==============================] - 12s 83ms/step - loss: 2.7302 - accuracy: 0.2482\n","Epoch 24/200\n","143/143 [==============================] - 11s 79ms/step - loss: 2.6852 - accuracy: 0.2509\n","Epoch 25/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.6357 - accuracy: 0.2540\n","Epoch 26/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.5859 - accuracy: 0.2580\n","Epoch 27/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.5382 - accuracy: 0.2607\n","Epoch 28/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.4797 - accuracy: 0.2660\n","Epoch 29/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.4279 - accuracy: 0.2697\n","Epoch 30/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.3625 - accuracy: 0.2754\n","Epoch 31/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.3047 - accuracy: 0.2808\n","Epoch 32/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.2414 - accuracy: 0.2869\n","Epoch 33/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.1766 - accuracy: 0.2949\n","Epoch 34/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.1250 - accuracy: 0.3013\n","Epoch 35/200\n","143/143 [==============================] - 11s 78ms/step - loss: 2.0779 - accuracy: 0.3067\n","Epoch 36/200\n","143/143 [==============================] - 11s 79ms/step - loss: 2.0229 - accuracy: 0.3137\n","Epoch 37/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.9799 - accuracy: 0.3196\n","Epoch 38/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.9338 - accuracy: 0.3258\n","Epoch 39/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.8945 - accuracy: 0.3319\n","Epoch 40/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.8585 - accuracy: 0.3373\n","Epoch 41/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.8213 - accuracy: 0.3412\n","Epoch 42/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.7852 - accuracy: 0.3475\n","Epoch 43/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.7552 - accuracy: 0.3527\n","Epoch 44/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.7275 - accuracy: 0.3572\n","Epoch 45/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.7005 - accuracy: 0.3611\n","Epoch 46/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.6753 - accuracy: 0.3651\n","Epoch 47/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.6541 - accuracy: 0.3693\n","Epoch 48/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.6339 - accuracy: 0.3727\n","Epoch 49/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.6139 - accuracy: 0.3767\n","Epoch 50/200\n","143/143 [==============================] - 12s 81ms/step - loss: 1.5937 - accuracy: 0.3805\n","Epoch 51/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.5781 - accuracy: 0.3820\n","Epoch 52/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.5568 - accuracy: 0.3856\n","Epoch 53/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.5422 - accuracy: 0.3881\n","Epoch 54/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.5336 - accuracy: 0.3897\n","Epoch 55/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.5174 - accuracy: 0.3934\n","Epoch 56/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.5010 - accuracy: 0.3961\n","Epoch 57/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.4896 - accuracy: 0.3977\n","Epoch 58/200\n","143/143 [==============================] - 12s 85ms/step - loss: 1.4780 - accuracy: 0.3999\n","Epoch 59/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.4689 - accuracy: 0.4016\n","Epoch 60/200\n","143/143 [==============================] - 11s 77ms/step - loss: 1.4545 - accuracy: 0.4043\n","Epoch 61/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.4506 - accuracy: 0.4062\n","Epoch 62/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.4358 - accuracy: 0.4070\n","Epoch 63/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.4286 - accuracy: 0.4094\n","Epoch 64/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.4195 - accuracy: 0.4115\n","Epoch 65/200\n","143/143 [==============================] - 11s 76ms/step - loss: 1.4133 - accuracy: 0.4121\n","Epoch 66/200\n","143/143 [==============================] - 11s 77ms/step - loss: 1.4002 - accuracy: 0.4149\n","Epoch 67/200\n","143/143 [==============================] - 11s 77ms/step - loss: 1.3949 - accuracy: 0.4155\n","Epoch 68/200\n","143/143 [==============================] - 12s 83ms/step - loss: 1.3875 - accuracy: 0.4174\n","Epoch 69/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.3783 - accuracy: 0.4197\n","Epoch 70/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.3732 - accuracy: 0.4204\n","Epoch 71/200\n","143/143 [==============================] - 14s 96ms/step - loss: 1.3758 - accuracy: 0.4189\n","Epoch 72/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.3652 - accuracy: 0.4218\n","Epoch 73/200\n","143/143 [==============================] - 12s 87ms/step - loss: 1.3559 - accuracy: 0.4250\n","Epoch 74/200\n","143/143 [==============================] - 14s 98ms/step - loss: 1.3474 - accuracy: 0.4249\n","Epoch 75/200\n","143/143 [==============================] - 14s 96ms/step - loss: 1.3490 - accuracy: 0.4248\n","Epoch 76/200\n","143/143 [==============================] - 12s 85ms/step - loss: 1.3452 - accuracy: 0.4266\n","Epoch 77/200\n","143/143 [==============================] - 15s 102ms/step - loss: 1.3387 - accuracy: 0.4273\n","Epoch 78/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.3356 - accuracy: 0.4278\n","Epoch 79/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.3331 - accuracy: 0.4279\n","Epoch 80/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.3239 - accuracy: 0.4299\n","Epoch 81/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.3251 - accuracy: 0.4295\n","Epoch 82/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.3149 - accuracy: 0.4327\n","Epoch 83/200\n","143/143 [==============================] - 13s 91ms/step - loss: 1.3147 - accuracy: 0.4327\n","Epoch 84/200\n","143/143 [==============================] - 12s 83ms/step - loss: 1.3108 - accuracy: 0.4336\n","Epoch 85/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.3099 - accuracy: 0.4330\n","Epoch 86/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.3094 - accuracy: 0.4335\n","Epoch 87/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.2982 - accuracy: 0.4366\n","Epoch 88/200\n","143/143 [==============================] - 14s 98ms/step - loss: 1.2946 - accuracy: 0.4365\n","Epoch 89/200\n","143/143 [==============================] - 14s 94ms/step - loss: 1.2941 - accuracy: 0.4362\n","Epoch 90/200\n","143/143 [==============================] - 12s 81ms/step - loss: 1.2902 - accuracy: 0.4362\n","Epoch 91/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.2846 - accuracy: 0.4388\n","Epoch 92/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.2832 - accuracy: 0.4400\n","Epoch 93/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.2785 - accuracy: 0.4403\n","Epoch 94/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2766 - accuracy: 0.4405\n","Epoch 95/200\n","143/143 [==============================] - 13s 89ms/step - loss: 1.2790 - accuracy: 0.4405\n","Epoch 96/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2785 - accuracy: 0.4405\n","Epoch 97/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2688 - accuracy: 0.4418\n","Epoch 98/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2652 - accuracy: 0.4429\n","Epoch 99/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2684 - accuracy: 0.4407\n","Epoch 100/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.2643 - accuracy: 0.4428\n","Epoch 101/200\n","143/143 [==============================] - 12s 81ms/step - loss: 1.2634 - accuracy: 0.4426\n","Epoch 102/200\n","143/143 [==============================] - 13s 94ms/step - loss: 1.2603 - accuracy: 0.4434\n","Epoch 103/200\n","143/143 [==============================] - 12s 85ms/step - loss: 1.2598 - accuracy: 0.4446\n","Epoch 104/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2557 - accuracy: 0.4443\n","Epoch 105/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.2535 - accuracy: 0.4454\n","Epoch 106/200\n","143/143 [==============================] - 13s 88ms/step - loss: 1.2497 - accuracy: 0.4457\n","Epoch 107/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2456 - accuracy: 0.4474\n","Epoch 108/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2491 - accuracy: 0.4455\n","Epoch 109/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2471 - accuracy: 0.4473\n","Epoch 110/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2387 - accuracy: 0.4492\n","Epoch 111/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.2427 - accuracy: 0.4469\n","Epoch 112/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2383 - accuracy: 0.4486\n","Epoch 113/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2359 - accuracy: 0.4488\n","Epoch 114/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.2361 - accuracy: 0.4488\n","Epoch 115/200\n","143/143 [==============================] - 14s 94ms/step - loss: 1.2370 - accuracy: 0.4486\n","Epoch 116/200\n","143/143 [==============================] - 14s 98ms/step - loss: 1.2292 - accuracy: 0.4510\n","Epoch 117/200\n","143/143 [==============================] - 13s 93ms/step - loss: 1.2350 - accuracy: 0.4496\n","Epoch 118/200\n","143/143 [==============================] - 13s 90ms/step - loss: 1.2329 - accuracy: 0.4507\n","Epoch 119/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2284 - accuracy: 0.4509\n","Epoch 120/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2245 - accuracy: 0.4522\n","Epoch 121/200\n","143/143 [==============================] - 12s 87ms/step - loss: 1.2240 - accuracy: 0.4524\n","Epoch 122/200\n","143/143 [==============================] - 13s 91ms/step - loss: 1.2244 - accuracy: 0.4523\n","Epoch 123/200\n","143/143 [==============================] - 13s 88ms/step - loss: 1.2278 - accuracy: 0.4517\n","Epoch 124/200\n","143/143 [==============================] - 13s 89ms/step - loss: 1.2186 - accuracy: 0.4531\n","Epoch 125/200\n","143/143 [==============================] - 13s 89ms/step - loss: 1.2192 - accuracy: 0.4533\n","Epoch 126/200\n","143/143 [==============================] - 15s 104ms/step - loss: 1.2208 - accuracy: 0.4529\n","Epoch 127/200\n","143/143 [==============================] - 12s 81ms/step - loss: 1.2185 - accuracy: 0.4535\n","Epoch 128/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2152 - accuracy: 0.4540\n","Epoch 129/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2141 - accuracy: 0.4535\n","Epoch 130/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.2108 - accuracy: 0.4541\n","Epoch 131/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2144 - accuracy: 0.4533\n","Epoch 132/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2097 - accuracy: 0.4544\n","Epoch 133/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2102 - accuracy: 0.4547\n","Epoch 134/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2126 - accuracy: 0.4541\n","Epoch 135/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2062 - accuracy: 0.4550\n","Epoch 136/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.2060 - accuracy: 0.4554\n","Epoch 137/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2047 - accuracy: 0.4558\n","Epoch 138/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2054 - accuracy: 0.4556\n","Epoch 139/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.2030 - accuracy: 0.4560\n","Epoch 140/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.2043 - accuracy: 0.4562\n","Epoch 141/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1988 - accuracy: 0.4565\n","Epoch 142/200\n","143/143 [==============================] - 12s 81ms/step - loss: 1.2021 - accuracy: 0.4557\n","Epoch 143/200\n","143/143 [==============================] - 14s 100ms/step - loss: 1.2007 - accuracy: 0.4569\n","Epoch 144/200\n","143/143 [==============================] - 13s 92ms/step - loss: 1.1993 - accuracy: 0.4563\n","Epoch 145/200\n","143/143 [==============================] - 13s 92ms/step - loss: 1.1970 - accuracy: 0.4573\n","Epoch 146/200\n","143/143 [==============================] - 12s 81ms/step - loss: 1.1987 - accuracy: 0.4573\n","Epoch 147/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1936 - accuracy: 0.4581\n","Epoch 148/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1957 - accuracy: 0.4575\n","Epoch 149/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1934 - accuracy: 0.4577\n","Epoch 150/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1941 - accuracy: 0.4580\n","Epoch 151/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1905 - accuracy: 0.4580\n","Epoch 152/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.1910 - accuracy: 0.4592\n","Epoch 153/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.1907 - accuracy: 0.4585\n","Epoch 154/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.1883 - accuracy: 0.4588\n","Epoch 155/200\n","143/143 [==============================] - 12s 82ms/step - loss: 1.1851 - accuracy: 0.4605\n","Epoch 156/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1879 - accuracy: 0.4598\n","Epoch 157/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1906 - accuracy: 0.4583\n","Epoch 158/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1870 - accuracy: 0.4597\n","Epoch 159/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1867 - accuracy: 0.4600\n","Epoch 160/200\n","143/143 [==============================] - 14s 96ms/step - loss: 1.1880 - accuracy: 0.4600\n","Epoch 161/200\n","143/143 [==============================] - 12s 85ms/step - loss: 1.1820 - accuracy: 0.4606\n","Epoch 162/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1822 - accuracy: 0.4599\n","Epoch 163/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1795 - accuracy: 0.4615\n","Epoch 164/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1840 - accuracy: 0.4601\n","Epoch 165/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1822 - accuracy: 0.4606\n","Epoch 166/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1801 - accuracy: 0.4598\n","Epoch 167/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.1796 - accuracy: 0.4611\n","Epoch 168/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1793 - accuracy: 0.4606\n","Epoch 169/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1752 - accuracy: 0.4624\n","Epoch 170/200\n","143/143 [==============================] - 13s 90ms/step - loss: 1.1722 - accuracy: 0.4629\n","Epoch 171/200\n","143/143 [==============================] - 13s 93ms/step - loss: 1.1792 - accuracy: 0.4611\n","Epoch 172/200\n","143/143 [==============================] - 13s 90ms/step - loss: 1.1768 - accuracy: 0.4621\n","Epoch 173/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1766 - accuracy: 0.4619\n","Epoch 174/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1761 - accuracy: 0.4619\n","Epoch 175/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1761 - accuracy: 0.4617\n","Epoch 176/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1741 - accuracy: 0.4618\n","Epoch 177/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1768 - accuracy: 0.4623\n","Epoch 178/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1718 - accuracy: 0.4628\n","Epoch 179/200\n","143/143 [==============================] - 12s 82ms/step - loss: 1.1708 - accuracy: 0.4632\n","Epoch 180/200\n","143/143 [==============================] - 11s 80ms/step - loss: 1.1719 - accuracy: 0.4632\n","Epoch 181/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.1715 - accuracy: 0.4634\n","Epoch 182/200\n","143/143 [==============================] - 12s 82ms/step - loss: 1.1675 - accuracy: 0.4638\n","Epoch 183/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1689 - accuracy: 0.4643\n","Epoch 184/200\n","143/143 [==============================] - 11s 76ms/step - loss: 1.1724 - accuracy: 0.4636\n","Epoch 185/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1682 - accuracy: 0.4637\n","Epoch 186/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.1672 - accuracy: 0.4644\n","Epoch 187/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.1709 - accuracy: 0.4634\n","Epoch 188/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.1658 - accuracy: 0.4641\n","Epoch 189/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.1685 - accuracy: 0.4634\n","Epoch 190/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1668 - accuracy: 0.4644\n","Epoch 191/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1653 - accuracy: 0.4648\n","Epoch 192/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.1625 - accuracy: 0.4645\n","Epoch 193/200\n","143/143 [==============================] - 11s 79ms/step - loss: 1.1623 - accuracy: 0.4651\n","Epoch 194/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1658 - accuracy: 0.4637\n","Epoch 195/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1660 - accuracy: 0.4652\n","Epoch 196/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1610 - accuracy: 0.4654\n","Epoch 197/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1593 - accuracy: 0.4659\n","Epoch 198/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1601 - accuracy: 0.4656\n","Epoch 199/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1605 - accuracy: 0.4663\n","Epoch 200/200\n","143/143 [==============================] - 11s 78ms/step - loss: 1.1621 - accuracy: 0.4647\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f05c416dd10>"]},"metadata":{},"execution_count":37}],"source":["EPOCHS = 200\n","model.fit(dataset, epochs=EPOCHS, verbose=1)"]},{"cell_type":"markdown","source":["- 꾸준히 accuracy가 오르긴하지만, overfitting은 아닌지 알 수가 없어서 답답하다\n","- 그리고 너무 조금씩 accuracy가 늘어나서.. 다음 개선시도에서는 파라미터 조정을 조금 더 해봐야겠다는 생각이 들었다."],"metadata":{"id":"FEwJpzQkeiy0"}},{"cell_type":"code","source":[""],"metadata":{"id":"S3LcHL8FDRZD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 5. 모델 평가하기\n","\n","Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다.\n","\n"],"metadata":{"id":"eDpQCABmDSzd"}},{"cell_type":"markdown","source":["- **예측(inference) 단계**는 기본적으로 다음과 같은 과정을 거친다.\n","\n","<br>\n","\n","1. **새로운 입력 문장**에 대해서는 <u>훈련 때와 동일한 전처리</u>를 거친다.\n","2. 입력 문장을 토크나이징하고, `START_TOKEN`과 `END_TOKEN`을 추가한다.\n","3. 패딩 마스킹과 룩 어헤드 마스킹을 계산한다.\n","4. 디코더는 입력 시퀀스로부터 다음 단어를 예측한다.\n","5. 디코더는 예측된 다음 단어를 기존의 입력 시퀀스에 추가하여 새로운 입력으로 사용한다.\n","6. `END_TOKEN`이 예측되거나 문장의 최대 길이에 도달하면 디코더는 동작을 멈춘다.\n","\n","<br>\n","\n","위의 과정을 모두 담은 `decoder_inference()` 함수를 만듭니다."],"metadata":{"id":"uVkHwEG-j_Qh"}},{"cell_type":"code","source":["def decoder_inference(sentence):\n","  sentence = preprocess_sentence(sentence)\n","\n","  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n","  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n","  sentence = tf.expand_dims(\n","      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n","\n","  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n","  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n","  output_sequence = tf.expand_dims(START_TOKEN, 0)\n","\n","  # 디코더의 인퍼런스 단계\n","  for i in range(MAX_LENGTH):\n","    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n","    predictions = model(inputs=[sentence, output_sequence], training=False)\n","    predictions = predictions[:, -1:, :]\n","\n","    # 현재 예측한 단어의 정수\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n","    if tf.equal(predicted_id, END_TOKEN[0]):\n","      break\n","\n","    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n","    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n","    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output_sequence, axis=0)\n","print(\"슝=3\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JlNl6t-6kgw5","executionInfo":{"status":"ok","timestamp":1645568851719,"user_tz":-540,"elapsed":492,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"5ea16d62-5f4e-4975-b58d-3e1630553394"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"markdown","source":["- 임의의 입력 문장에 대해서 `decoder_inference()` 함수를 호출하여 챗봇의 대답을 얻는 `sentence_generation()` 함수를 만든다."],"metadata":{"id":"CqmAcgx7klB7"}},{"cell_type":"code","source":["def sentence_generation(sentence):\n","  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n","  prediction = decoder_inference(sentence)\n","\n","  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n","  predicted_sentence = tokenizer.decode(\n","      [i for i in prediction if i < tokenizer.vocab_size])\n","\n","  print('입력 : {}'.format(sentence))\n","  print('출력 : {}'.format(predicted_sentence))\n","\n","  return predicted_sentence\n","print(\"슝=3\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxJeAb9bkiok","executionInfo":{"status":"ok","timestamp":1645568856718,"user_tz":-540,"elapsed":513,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"c3db678f-757c-4af6-a269-1b43c58d20e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}]},{"cell_type":"markdown","source":["임의의 문장으로부터 챗봇의 대답을 얻어봅시다."],"metadata":{"id":"HBlNY9MHktwM"}},{"cell_type":"code","source":["sentence_generation('안녕?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"hF1ZLu3PkrqW","executionInfo":{"status":"ok","timestamp":1645568864603,"user_tz":-540,"elapsed":2303,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"5db69206-cf96-4032-bb6d-3e8be3eeb0dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 안녕?\n","출력 : 좋은 친구를 두셨네요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'좋은 친구를 두셨네요 .'"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["sentence_generation('여행가고 싶다')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"8hpRtlL9k2yF","executionInfo":{"status":"ok","timestamp":1645568866201,"user_tz":-540,"elapsed":1614,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"c5b3e2ef-dca2-4f33-c7f8-a232f96aa177"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 여행가고 싶다\n","출력 : 좋은 친구를 두셨네요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'좋은 친구를 두셨네요 .'"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["sentence_generation('너무 힘들어')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"SV22aJVLgRJM","executionInfo":{"status":"ok","timestamp":1645568867620,"user_tz":-540,"elapsed":1437,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"65361454-3b13-49c3-f4ed-9a90af3a334a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 너무 힘들어\n","출력 : 좋은 친구를 두셨네요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'좋은 친구를 두셨네요 .'"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["sentence_generation('제주도로 여행 가고 싶다')"],"metadata":{"id":"NN24KVNbk4Lw","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1645568868833,"user_tz":-540,"elapsed":1225,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"4d59e753-7d70-4a6c-ed08-087ae2769deb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 제주도로 여행 가고 싶다\n","출력 : 좋은 친구를 두셨네요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'좋은 친구를 두셨네요 .'"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["sentence_generation('방학이 기다려져')"],"metadata":{"id":"mY1HceOLDTvT","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1645568870255,"user_tz":-540,"elapsed":1434,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"040a897e-3b6a-4755-cce0-81316c067f8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 방학이 기다려져\n","출력 : 좋은 친구를 두셨네요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'좋은 친구를 두셨네요 .'"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"o5CYw3I8GEmU","executionInfo":{"status":"ok","timestamp":1645570034111,"user_tz":-540,"elapsed":756,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"fdfb51f0-94ed-4043-f0b8-5df51abfdda3"},"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 다음주 방학이야\n","출력 : 잠시 쉬어도 돼요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'잠시 쉬어도 돼요 .'"]},"metadata":{},"execution_count":58}],"source":["sentence_generation('다음주 방학이야')"]},{"cell_type":"code","source":["sentence_generation('좋겠지?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"QCfqZKLhfgId","executionInfo":{"status":"ok","timestamp":1645568876976,"user_tz":-540,"elapsed":1952,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"ce992760-b1b5-4442-f603-6c98777a2e66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 좋겠지?\n","출력 : 좋은 친구를 두셨네요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'좋은 친구를 두셨네요 .'"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["- OMG.. $whyrano$..."],"metadata":{"id":"RVaWJx6UjVxT"}},{"cell_type":"code","source":[""],"metadata":{"id":"WQ5vFOxOhgE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 6. 개선시도\n","\n","\n"],"metadata":{"id":"bqFUtX0jcUTa"}},{"cell_type":"markdown","source":["## 6-1)\n","\n","- 첫번째 시도에서 파라미터를 원래 논문에서 지정된 값으로 바꿔서 해 보았는데, accuracy가 너무 심하게 떨어졌다.\n","- 아무래도 데이터셋이 많지 않아서 숫자를 크게 하는 것이 역효과였나 하는 생각이 들어서 파라미터를 더 작게 조정해 보았다.\n","\n","<br>\n","\n","- 앞부분에 있던 배치사이즈도 `64 → 16`으로 줄여봄\n","- `NUM_LAYERS` = 2\n","- `D_MODEL` = 128\n","- `UNITS` = 256\n","- `DROPOUT` = 0.3"],"metadata":{"id":"roIVrvbUpiFE"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3554,"status":"ok","timestamp":1645569219104,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"outputId":"b5c08eab-9a25-4520-ea73-6a3dbbd78d38","id":"Zcl7fkAJc8Qn"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," inputs (InputLayer)            [(None, None)]       0           []                               \n","                                                                                                  \n"," dec_inputs (InputLayer)        [(None, None)]       0           []                               \n","                                                                                                  \n"," enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n","                                                                                                  \n"," encoder (Functional)           (None, None, 128)    1309440     ['inputs[0][0]',                 \n","                                                                  'enc_padding_mask[0][0]']       \n","                                                                                                  \n"," look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n","                                e)                                                                \n","                                                                                                  \n"," dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n","                                                                                                  \n"," decoder (Functional)           (None, None, 128)    1442048     ['dec_inputs[0][0]',             \n","                                                                  'encoder[0][0]',                \n","                                                                  'look_ahead_mask[0][0]',        \n","                                                                  'dec_padding_mask[0][0]']       \n","                                                                                                  \n"," outputs (Dense)                (None, None, 8160)   1052640     ['decoder[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 3,804,128\n","Trainable params: 3,804,128\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["tf.keras.backend.clear_session()\n","\n","# 하이퍼파라미터\n","NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n","D_MODEL = 128 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n","NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n","UNITS = 256 # 피드 포워드 신경망의 은닉층의 크기\n","DROPOUT = 0.3 # 드롭아웃의 비율\n","\n","model = transformer(\n","    vocab_size=VOCAB_SIZE,\n","    num_layers=NUM_LAYERS,\n","    units=UNITS,\n","    d_model=D_MODEL,\n","    num_heads=NUM_HEADS,\n","    dropout=DROPOUT)\n","\n","model.summary()"]},{"cell_type":"markdown","source":["- `d_model` = 128 (위와 같이 맞춰준다)"],"metadata":{"id":"MMku5H8oeDsD"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"elapsed":884,"status":"ok","timestamp":1645569222736,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"outputId":"481a35f0-bfa5-49f6-a125-b8f74eacbad6","id":"PtwCCvbSc8Q5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'Train Step')"]},"metadata":{},"execution_count":55},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["sample_learning_rate = CustomSchedule(d_model=64)\n","\n","plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n","plt.ylabel(\"Learning Rate\")\n","plt.xlabel(\"Train Step\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1645569223419,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"},"user_tz":-540},"outputId":"7b675fa3-75ca-4004-e56e-54fb8be74eba","id":"fMYu5L7Cc8Q_"},"outputs":[{"output_type":"stream","name":"stdout","text":["슝=3\n"]}],"source":["learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","def accuracy(y_true, y_pred):\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n","\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n","print(\"슝=3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":700},"outputId":"a735abae-619f-4c33-8e46-0991d4df3277","id":"wEpzVpXWc8RC","executionInfo":{"status":"error","timestamp":1645570193832,"user_tz":-540,"elapsed":149359,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","570/570 [==============================] - 16s 28ms/step - loss: 1.2934 - accuracy: 0.4646\n","Epoch 2/50\n","570/570 [==============================] - 16s 28ms/step - loss: 1.2878 - accuracy: 0.4660\n","Epoch 3/50\n","570/570 [==============================] - 16s 28ms/step - loss: 1.2876 - accuracy: 0.4656\n","Epoch 4/50\n","570/570 [==============================] - 16s 28ms/step - loss: 1.2889 - accuracy: 0.4657\n","Epoch 5/50\n","570/570 [==============================] - 16s 28ms/step - loss: 1.2767 - accuracy: 0.4673\n","Epoch 6/50\n","570/570 [==============================] - 16s 28ms/step - loss: 1.2804 - accuracy: 0.4658\n","Epoch 7/50\n","570/570 [==============================] - 16s 28ms/step - loss: 1.2729 - accuracy: 0.4662\n","Epoch 8/50\n","570/570 [==============================] - 16s 28ms/step - loss: 1.2645 - accuracy: 0.4678\n","Epoch 9/50\n","570/570 [==============================] - 16s 27ms/step - loss: 1.2597 - accuracy: 0.4681\n","Epoch 10/50\n","239/570 [===========>..................] - ETA: 9s - loss: 1.1765 - accuracy: 0.4751"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-5603f852643a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["EPOCHS = 50\n","model.fit(dataset, epochs=EPOCHS, verbose=1)"]},{"cell_type":"markdown","source":["- 50 에폭을 돌렸는데, accuracy가 0.46정도에서 더 올라가질 않아서.. 한번 더 돌려보았지만 의미가 없는 듯하여 중단\n","- 첫번째 시도와 마찬가지로 조금씩 accuracy가 증가하는 모양새지만, 그래도 그와 비교해서 훨씬 적은 에폭으로 비슷한 accuracy가 나왔다는 것에 만족스럽다."],"metadata":{"id":"KuthZRTMliZx"}},{"cell_type":"code","source":[""],"metadata":{"id":"9Gf4qV-Ulh3m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**모델 평가**"],"metadata":{"id":"NishinZpe__1"}},{"cell_type":"code","source":["sentence_generation('안녕?')"],"metadata":{"executionInfo":{"status":"ok","timestamp":1645570205582,"user_tz":-540,"elapsed":835,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"HDl5XJ12fd4b","outputId":"d7403d9c-708b-4d1b-f716-bd7d24a483cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 안녕?\n","출력 : 힘든 결정이었을텐데 맘고생 많았어요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'힘든 결정이었을텐데 맘고생 많았어요 .'"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["sentence_generation('여행가고 싶다')"],"metadata":{"executionInfo":{"status":"ok","timestamp":1645570210258,"user_tz":-540,"elapsed":1216,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"pOL0uy2Tfd4f","outputId":"63189bb9-18b6-4fa6-ace9-0d5975a6ff5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 여행가고 싶다\n","출력 : 자신을 먼저 키우세요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'자신을 먼저 키우세요 .'"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["sentence_generation('너무 힘들어')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"mbgfjdp4gXGu","executionInfo":{"status":"ok","timestamp":1645570214009,"user_tz":-540,"elapsed":1190,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"43b287cc-7590-4959-e049-e17aa389dd2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 너무 힘들어\n","출력 : 더 열심히 노력하겠습니다 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'더 열심히 노력하겠습니다 .'"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["sentence_generation('제주도로 여행 가고 싶다')"],"metadata":{"executionInfo":{"status":"ok","timestamp":1645570218805,"user_tz":-540,"elapsed":1160,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"a-Hzw-p-gXGw","outputId":"9ca110cf-3650-449c-bbe1-fb5cb59efe53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 제주도로 여행 가고 싶다\n","출력 : 더 좋은 기회가 올 거예요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'더 좋은 기회가 올 거예요 .'"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["sentence_generation('방학이 기다려져')"],"metadata":{"executionInfo":{"status":"ok","timestamp":1645570223031,"user_tz":-540,"elapsed":1288,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"Mx-RhDX2fd4i","outputId":"c3d05669-3c22-46f7-973b-3296fbfaf2ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 방학이 기다려져\n","출력 : 잠깐 핸드폰 게임 하시면 올거예요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'잠깐 핸드폰 게임 하시면 올거예요 .'"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"status":"ok","timestamp":1645570226596,"user_tz":-540,"elapsed":543,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"PXQNgcm5fd4j","outputId":"6870844b-32fe-4be5-c271-4c12d6b32181"},"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 나 다음주 방학이야\n","출력 : 그러게 말이에요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'그러게 말이에요 .'"]},"metadata":{},"execution_count":67}],"source":["sentence_generation('나 다음주 방학이야')"]},{"cell_type":"code","source":["sentence_generation('좋겠지?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"NvcEgKRVhiaJ","executionInfo":{"status":"ok","timestamp":1645570230666,"user_tz":-540,"elapsed":1163,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"c630d257-5ebe-4902-e0e4-12f3c023dcbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 : 좋겠지?\n","출력 : 너무 신경쓰지마세요 .\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'너무 신경쓰지마세요 .'"]},"metadata":{},"execution_count":68}]},{"cell_type":"markdown","source":["- 첫번째 시도와 최종 accuracy는 비슷한데 전혀 다른 결과가 나왔다! 🎉\n","- 여전히 동문서답으로 느껴지긴 하지만.. **'방학이 기다려져'**의 대답으로 **'잠깐 핸드폰 게임 하시면 올거예요 .'**라는 출력이 돌아왔는데, 개인적으로는 <u>'기다린다'</u>라는 말에 대한 답변이라고 생각되어 상당히 고무적이다.\n","- **너무 힘들어** 👉 **더 열심히 노력하겠습니다.** : <u>힘들어</u>에 대한 답변으로 보인다.\n","- **좋겠지?** 👉 **너무 신경쓰지마세요** : <u>~겠지?</u> 라는 말에 대한 답변으로 보임\n","\n","😆 csv 파일을 출력해서 봤을때 질문과 답변을 고려해보면 상당히 괜찮은 출력이라고 생각한다❗"],"metadata":{"id":"Hk4nfBZ2ojp0"}},{"cell_type":"code","source":[""],"metadata":{"id":"C0mpvL56hi8P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 회고"],"metadata":{"id":"ioooKKmBnCIa"}},{"cell_type":"markdown","source":["## 공부한 내용"],"metadata":{"id":"tzUEt9Q5oBZ7"}},{"cell_type":"markdown","source":["### 1. 그래프 그리기\n","\n","- **MAX_LENGTH**를 구하기 위해서 막대그래프로 시각화를 해 보았다.\n","- `plt.subplot()`를 사용해보고 싶었는데, 그냥 단순하다고 생각했던 코드가 막상 내가 코드를 입력하니 잘 되지 않았다.\n","- 빈 그래프가 생성된 뒤에 생겨나기도 하고, 2개의 그래프가 제대로 출력되지 않고 계속 순차적으로 출력되는 등의 문제가 생겼다.\n","\n","<br>\n","\n","👉 `plt.show()`가 매 그래프마다 입력되어있으면, 순차적으로 출력된다는 것을 오늘에서야 알게되었다... \n","\n","`plt.show()`를 마지막에 한 번만 사용하던가, 굳이 쓰지 않아도 생각한 대로 출력되었다."],"metadata":{"id":"lpPdVWCvoDda"}},{"cell_type":"markdown","source":["### 2. `validation_split`\n","\n","- 모델 학습을 할때, 50에폭으로는 accuracy가 0.33정도 밖에 나오지 않았다.\n","- 그래서 에폭을 많이 늘려서 학습을 하면 어떨까, 하는 생각이 들었는데 그러면 과적합이 일어나는건 아닐까 하는 걱정이 들었음\n","- `validation_split=0.2`를 옵션으로 추가해 보았는데, 다음과 같은 에러가 떴다.\n","\n","![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c94a8801-9b06-4bd0-b1e4-3fd9666ebd32/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220222%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220222T214035Z&X-Amz-Expires=86400&X-Amz-Signature=9d44fe9df9057d0412e0ebb69422a899f4d7d9245ffa5248e4ed5fb4d0e32e62&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n","\n","- [stack overflow](https://stackoverflow.com/questions/63909553/what-is-tensorflow-python-data-ops-dataset-ops-prefetchdataset)에 검색한 내용을 보니, `dataset_ops.prefetchdataset`은 iterator로 인식이 되지 않는 모양이었다.\n","\n","```\n","BATCH_SIZE = 64\n","BUFFER_SIZE = len(questions)    # buffer size를 샘플의 갯수로 조정한다.\n","\n","# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n","# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n","dataset = tf.data.Dataset.from_tensor_slices((\n","    { 'inputs': questions,\n","     'dec_inputs': answers[:, :-1]},\n","    {'outputs': answers[:, 1:]}))\n","\n","dataset = dataset.cache()\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE)\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","```\n","\n","- 그래서 교사 강요를 적용했던 위의 코드에서 마지막 코드를 주석처리하고 실행해 보았으나, 이번엔 `dataset.batch`가 적용되지 않는다고 했다. \n","- 아마도 `tf.data.Dataset.from_tensor_slices()`이후의 `dataset` 처리를 한 것들이 iterator에서 벗어나게 한 것은 아닐까 추측하고 있다."],"metadata":{"id":"fpVL3AR2LgoC"}},{"cell_type":"code","source":[""],"metadata":{"id":"fGeCKQUxfsEX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 루브릭 평가 지표를 맞추기 위해 시도한 것들"],"metadata":{"id":"TMtixwVfsRI6"}},{"cell_type":"markdown","source":["|평가문항|상세기준|\n","|:------|:---|\n","|1. 한국어 전처리를 통해 학습 데이터셋을 구축하였다.|공백과 특수문자 처리, 토크나이징, 병렬데이터 구축의 과정이 적절히 진행되었다.|\n","|2. 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행하였다.|구현한 트랜스포머 모델이 한국어 병렬 데이터 학습 시 안정적으로 수렴하였다.|\n","|3. 한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다.|한국어 입력문장에 그럴듯한 한국어로 답변을 리턴하였다.|"],"metadata":{"id":"lRhHhQbZnOUQ"}},{"cell_type":"markdown","source":["- 모두 완료\n","- 첫번째는 폭망했지만, 두번째 시도에서는 훨씬 적은 에폭으로 비슷한 수준의 accracy가 나왔고, 모델의 대답 또한 만족스러웠다.\n"],"metadata":{"id":"ep2YPl2pfuUw"}},{"cell_type":"code","source":[""],"metadata":{"id":"YLDr_beshv55"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 만약에 루브릭 평가 관련 지표를 달성 하지 못했을 때, 이유에 관한 추정\n","\n","    "],"metadata":{"id":"Nzvd3OucmQpS"}},{"cell_type":"markdown","source":["- 어려웠다... 그냥 따라 하는 것은 쉬웠지만, 개념을 코드로 구현하는 것을 이해하는건 어렵다고 생각했다.\n","- 파라미터를 조절했더니 왜인지 원래보다 더 accuracy가 낮게 나와서 조금 당황했다.\n","- 더욱 놀란 것은 첫번째 시도했던 학습의 결과... 너무 충격적이었다.... $whyrano....$ (왜 논문 내용이라고 해서 따라하면 다 실패하는 건지? 라는 근원적인 의문이 들었다. ~폐렴 익플때도 레즈넷 좋대서 썼더니만 recall 1 나오던거 잊지 못해.. 걔가 분류를 잘한다면서요...~)\n"],"metadata":{"id":"i0uYqxW6fzr7"}},{"cell_type":"code","source":[""],"metadata":{"id":"6E2bPe76h8Uz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 궁금해서 추가로 읽어본 것\n","\n","- 시간이 없어서 다른 개선은 시도해보지 못했지만, <u>내가 시도한 두 모델이 *accuracy는 비슷했으나* 출력된 결과물에 상당한 차이가 있었던 것이 의아</u>하기도 해서 명예의 전당 아이펠 선배님들의 노드를 보고 ***파라미터 변화값에 따른 회고***를 읽어보았다.\n","- 다른 분들의 회고에서도, accuracy와 같은 metric 지수가 높게 나와도 결과가 형편없는 것이 많이 있었다. (나의 실패보다야 훨씬 봐줄만 했지만...8ㅅ8)\n","- 나의 두 모델도 ***첫번째는 파라미터 값을 높게*** 잡았고, ***두번째는 파라미터 값을 많이 줄인 것***이 차이점이었는데, <u>똑같은 내용의 회고가 있었다.</u> 참고 했었던 **서태원님**의 회고에 따르면, 처음에는 accuracy가 높게 나와서 결과값이 좋으리라 기대했지만, 생각외로 결과가 너무 좋지 않았다고 한다. 그래서 모든 파라미터값을 가볍게 교체했더니, accuracy는 조금 낮게 나왔지만 챗봇의 대답은 훨씬 자연스러워 졌다고 한다.\n","- 그 외에도, 내 모델의 accuracy보다 낮은 데도 불구하고 꽤나 자연스러운 대답이 나온 노드도 있었고... 신기하긴 한데, 어디가 어떻게 잘못되었고 어떻게 잘 된 건지 가늠을 할 수 없었던 것이 답답한 부분이다."],"metadata":{"id":"_y8lCFferNLe"}},{"cell_type":"code","source":[""],"metadata":{"id":"dnoVZ62huI4z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 자기 다짐\n","\n","- nlp와 cv를 아직 고민중인데... 점점 nlp의 내용이 따라가기 벅찬것 같다. 욕심으로는 둘 다 그래도 어느 정도는 해두고 싶은 마음이지만... 고민이 많은 익플"],"metadata":{"id":"94eg-dX_mM9n"}}]}