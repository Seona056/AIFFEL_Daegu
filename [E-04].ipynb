{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[E-04].ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1fAd_XiTqYiHIhxvpeaMPRelmfUlOXTvC","authorship_tag":"ABX9TyPTvSmlEstsVOwWxohEPs3K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 4-7. 프로젝트: 멋진 작사가 만들기\n","\n","**Step 1. 데이터 다운로드**\n","\n","이미 `실습(1) 데이터 다듬기`에서 `Cloud shell`에 심볼릭 링크로 `~/aiffel/lyricist/data`를 생성하셨다면, `~/aiffel/lyricist/data/lyrics`에 데이터가 있습니다.\n","\n","**Step 2. 데이터 읽어오기**\n","\n","`glob` 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이해요. `glob` 를 활용하여 모든 txt 파일을 읽어온 후, `raw_corpus` 리스트에 문장 단위로 저장하도록 할게요!"],"metadata":{"id":"Fziuv4YM2Rrq"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ScCZZEOhrQb","executionInfo":{"status":"ok","timestamp":1642433816620,"user_tz":-540,"elapsed":12297,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"054e7e97-38cd-4a82-fcbe-087ab5513e51"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import glob\n","import os\n","\n","txt_file_path = '/content/drive/MyDrive/아이펠/풀잎스쿨/data/lyricist/data/lyrics/*'\n","\n","txt_list = glob.glob(txt_file_path)\n","\n","raw_corpus = []\n","\n","# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담는다.\n","# raw_corpus에는 문장 단위로 데이터가 들어간다.\n","for txt_file in txt_list:\n","    with open(txt_file, \"r\") as f:\n","        raw = f.read().splitlines()\n","        raw_corpus.extend(raw)\n","\n","print(\"데이터 크기(문장 갯수):\", len(raw_corpus))\n","print(\"Examples:\\n\", raw_corpus[:3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lgk2VRyE2Lop","executionInfo":{"status":"ok","timestamp":1642433847961,"user_tz":-540,"elapsed":31350,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"8f86b81b-19d4-4105-ab7c-4e66d91eb118"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["데이터 크기(문장 갯수): 187088\n","Examples:\n"," ['Hey, Vietnam, Vietnam, Vietnam, Vietnam', 'Vietnam, Vietnam, Vietnam Yesterday I got a letter from my friend', 'Fighting in Vietnam']\n"]}]},{"cell_type":"markdown","source":["**Step 3. 데이터 정제**\n","\n","앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n","\n","`preprocess_sentence()` 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n","\n","추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 있겠죠.\n","그래서 이번에는 문장을 토큰화 했을 때 **토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기** 를 권합니다."],"metadata":{"id":"BtFsojGt2ygi"}},{"cell_type":"code","source":["import re\n","# 입력된 문장을\n","#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n","#     2. 특수문자 양쪽에 공백을 넣고\n","#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n","#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n","#     5. 다시 양쪽 공백을 지웁니다\n","#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n","# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip() # 1 # strip()은 문자열 양쪽 끝에 있는 공백을 제거\n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2 # \\1은 첫번째 그룹을 뜻한다.\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n","    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n","    sentence = sentence.strip() # 5 \n","    sentence = '<start> ' + sentence + ' <end>' # 6\n","    return sentence"],"metadata":{"id":"sBBZj5ZJjumM","executionInfo":{"status":"ok","timestamp":1642433847963,"user_tz":-540,"elapsed":13,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# corpus 정제된 문장을 담을 리스트\n","corpus = []\n","\n","# preprocess_sentence 함수를 이용해서 정제된 문장을 담아준다.\n","# 토큰의 개수가 15개를 넘어가는 문장을 제외\n","# split 합수로 공백을 기준으로 단어를 나누어 준다.\n","for sentence in raw_corpus:\n","    if len(sentence) == 0:\n","        continue\n","    if len(sentence.split()) >= 13:  # 토큰 15개 이상이면 삭제하라고 되어있는데, 그 뒤에 학습데이터수가 124960개 이상이면 안된다고 해서 다시 줄임\n","        continue\n","    corpus.append(preprocess_sentence(sentence))\n","        \n","# 정제된 결과를 10개 확인\n","corpus[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gBnlYPjkdEI","executionInfo":{"status":"ok","timestamp":1642433849566,"user_tz":-540,"elapsed":1612,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"187bf6e6-d211-4aff-e51b-8fbe11745444"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> hey , vietnam , vietnam , vietnam , vietnam <end>',\n"," '<start> vietnam , vietnam , vietnam yesterday i got a letter from my friend <end>',\n"," '<start> fighting in vietnam <end>',\n"," '<start> and this is what he had to say <end>',\n"," '<start> tell all my friends that i ll be coming home soon <end>',\n"," '<start> my time it ll be up some time in june <end>',\n"," '<start> don t forget , he said to tell my sweet mary <end>',\n"," '<start> her golden lips as sweet as cherries and it came from <end>',\n"," '<start> vietnam , vietnam , vietnam , vietnam <end>',\n"," '<start> it was addressed from vietnam <end>']"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# 토큰화\n","\n","import tensorflow as tf\n","import numpy as np\n","\n","def tokenize(corpus):\n","    # 12000단어를 기억할 수 있는 tokenizer를 만들겁니다 (전체 단어의 수 12000개로 제한)\n","    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n","    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=12000, \n","        filters=' ',\n","        oov_token=\"<unk>\"\n","    )\n","\n","    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n","    # fit_on_texts는 리스트 형태로 결과를 반환\n","    tokenizer.fit_on_texts(corpus)\n","    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n","    tensor = tokenizer.texts_to_sequences(corpus)   \n"," \n","    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n","    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n","    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n","    \n","    print(tensor,tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)\n","\n","print('\\n-------------------------\\n')\n","print('텐서의 갯수:', len(tensor))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klxY6EsRm7u9","executionInfo":{"status":"ok","timestamp":1642433854794,"user_tz":-540,"elapsed":5232,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"2af16813-1636-4330-bf22-09425cd63ac6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[   2  141    4 ...    0    0    0]\n"," [   2 1296    4 ...    0    0    0]\n"," [   2 1072   14 ...    0    0    0]\n"," ...\n"," [   2   44    6 ...    0    0    0]\n"," [   2   31    7 ...    0    0    0]\n"," [   2  304    1 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f9987539c10>\n","\n","-------------------------\n","\n","텐서의 갯수: 158876\n"]}]},{"cell_type":"markdown","source":["**Step 4. 평가 데이터셋 분리**\n","\n","훈련 데이터와 평가 데이터를 분리하세요!\n","\n","`tokenize()` 함수로 데이터를 Tensor로 변환한 후, `sklearn` 모듈의 `train_test_split()` 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. `단어장의 크기는 12,000 이상` 으로 설정하세요! **총 데이터의 20%** 를 평가 데이터셋으로 사용해 주세요!"],"metadata":{"id":"V8f2LrRP26Hm"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","src_input = tensor[:, :-1]  # 행 전체, 마지막 열을 제외한 모든 열\n","tgt_input = tensor[:, 1:]   # 행 전체, 첫번째 열을 제외한 모든 열 (<start>제외)\n","\n","enc_train, enc_val, dec_train, dec_val = train_test_split(\n","    src_input, tgt_input,\n","    random_state=2022,\n","    test_size = 0.2\n",")"],"metadata":{"id":"4VaWh_LpWzwO","executionInfo":{"status":"ok","timestamp":1642433855249,"user_tz":-540,"elapsed":466,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["여기까지 올바르게 진행했을 경우, 아래 실행 결과를 확인할 수 있습니다."],"metadata":{"id":"eTcB-1Vo3QJf"}},{"cell_type":"code","source":["# encoding, decoding의 약자인듯. 갑자기 변수명이 바뀌어서 헷갈렸다.\n","print(\"Source Train:\", enc_train.shape)\n","print(\"Target Train:\", dec_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MrXbYX5X3Oft","executionInfo":{"status":"ok","timestamp":1642433855250,"user_tz":-540,"elapsed":8,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"51c43e0f-dbd5-4f1e-95c1-32d74ec753a7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Source Train: (127100, 32)\n","Target Train: (127100, 32)\n"]}]},{"cell_type":"code","source":["# shuffle에서 쓰인다.\n","# shuffle 함수는 고정된 버퍼 크기로 데이터를 섞는데, \n","# 완전히 랜덤하게 섞기 위해서는 입력된 데이터 크기보다 큰 수를 입력해 주어야 한다.\n","# 지금은 train 데이터셋 전체 갯수(127100)를 buffer_size로 설정했다.\n","BUFFER_SIZE = len(enc_train)\n","# batch_size : 한 번에 읽어올 데이터의 갯수\n","BATCH_SIZE = 256\n","# 여기서 정한 steps_per_epoch은 뒤에 사용되지 않았다.\n","steps_per_epoch = len(enc_train) // BATCH_SIZE\n","\n","# tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","# 앞에서 num_words = 12000으로 정해주었음\n","VOCAB_SIZE = tokenizer.num_words + 1\n","\n","# tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환\n","# Premade Estimator를 사용하기 위해서는 feature 데이터와 label 데이터가 함께 전달하여 dataset을 생성해야 한다.\n","train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n","# drop_remainder=True는 마지막 남은 데이터를 버린다는 옵션 설정\n","# 127100 // 256 : 나머지 데이터는 버림\n","train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder = True)\n","train_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R9dujGijCLL9","executionInfo":{"status":"ok","timestamp":1642433858359,"user_tz":-540,"elapsed":3114,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"78fdab77-4022-4d11-e294-d116aa0bd5f6"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((256, 32), (256, 32)), types: (tf.int32, tf.int32)>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# validation 데이터셋도 텐서를 tf.data.Dataset 객체로 변환시켜준다.\n","test_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n","test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder = True)\n","test_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1PwuOJZFEhg5","executionInfo":{"status":"ok","timestamp":1642433858361,"user_tz":-540,"elapsed":42,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"38302324-a1a3-4f39-eef6-1d2c931aca3d"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((256, 32), (256, 32)), types: (tf.int32, tf.int32)>"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["만약 결과가 다르다면 천천히 과정을 다시 살펴 동일한 결과를 얻도록 하세요! 만약 학습 데이터 개수가 124960보다 크다면 위 Step 3.의 데이터 정제 과정을 다시 한번 검토해 보시기를 권합니다."],"metadata":{"id":"jMyTkJnb3TZ9"}},{"cell_type":"markdown","source":["**Step 5. 인공지능 만들기**\n","\n","모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n","\n","그리고 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!"],"metadata":{"id":"Khuhjga43VJg"}},{"cell_type":"code","source":["class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","        \n","        # embedding_size : 워드벡터의 차원 수\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","        \n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.linear(out)\n","        \n","        return out\n","    \n","embedding_size = 256\n","hidden_size = 1024\n","model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"],"metadata":{"id":"JlXg3l03GPzz","executionInfo":{"status":"ok","timestamp":1642433858361,"user_tz":-540,"elapsed":37,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["for src_sample, tgt_sample in train_dataset.take(1):\n","    break\n","\n","# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n","model(src_sample)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mboYEQWhH-VE","executionInfo":{"status":"ok","timestamp":1642433865734,"user_tz":-540,"elapsed":7409,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"c6c7dc8d-f75e-4c88-ea6c-c8dcfe7f3534"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(256, 32, 12001), dtype=float32, numpy=\n","array([[[ 3.30475654e-04,  1.44331323e-04, -1.80458257e-04, ...,\n","          5.72627650e-05,  1.89925195e-04, -5.20303838e-05],\n","        [ 5.45686751e-04,  2.77402432e-04, -2.06524841e-04, ...,\n","          1.51327302e-04,  2.20560847e-04,  1.64475623e-05],\n","        [ 5.07978722e-04,  3.62450606e-04, -1.52246706e-04, ...,\n","          2.24055548e-04,  2.74231279e-04,  1.81390365e-04],\n","        ...,\n","        [-3.81102413e-03, -1.57169509e-03, -2.70972052e-03, ...,\n","         -2.07704306e-03,  4.47048945e-03, -3.54059716e-03],\n","        [-3.82430037e-03, -1.56424055e-03, -2.72766664e-03, ...,\n","         -2.12893402e-03,  4.51802835e-03, -3.54226422e-03],\n","        [-3.83470999e-03, -1.55552232e-03, -2.74355640e-03, ...,\n","         -2.17411807e-03,  4.55840491e-03, -3.54350242e-03]],\n","\n","       [[ 3.30475654e-04,  1.44331323e-04, -1.80458257e-04, ...,\n","          5.72627650e-05,  1.89925195e-04, -5.20303838e-05],\n","        [ 5.80939348e-04,  5.84387853e-05, -2.52813217e-04, ...,\n","          2.40573383e-04,  4.43727447e-04,  8.60213186e-06],\n","        [ 5.96108031e-04, -7.36844813e-05,  1.39711410e-05, ...,\n","          8.69247378e-05,  4.05676750e-04, -2.16757020e-04],\n","        ...,\n","        [-3.93965375e-03, -1.53491285e-03, -2.72678630e-03, ...,\n","         -2.10736273e-03,  4.55122208e-03, -3.53185786e-03],\n","        [-3.93317547e-03, -1.53063517e-03, -2.74357479e-03, ...,\n","         -2.15854635e-03,  4.58803168e-03, -3.53507581e-03],\n","        [-3.92670743e-03, -1.52504386e-03, -2.75833416e-03, ...,\n","         -2.20224890e-03,  4.61868010e-03, -3.53788305e-03]],\n","\n","       [[ 3.30475654e-04,  1.44331323e-04, -1.80458257e-04, ...,\n","          5.72627650e-05,  1.89925195e-04, -5.20303838e-05],\n","        [ 5.35845174e-04,  5.24791758e-05, -2.73499056e-04, ...,\n","          2.21624970e-04,  5.74658567e-04, -1.88648162e-04],\n","        [ 7.73376960e-04,  4.26338056e-06, -3.48638569e-04, ...,\n","          4.89053433e-04,  9.34657408e-04, -4.31341323e-04],\n","        ...,\n","        [-3.77681036e-03, -1.49892201e-03, -2.64037214e-03, ...,\n","         -1.84320868e-03,  4.24676808e-03, -3.51188704e-03],\n","        [-3.80283920e-03, -1.51526125e-03, -2.66606803e-03, ...,\n","         -1.92620209e-03,  4.32703644e-03, -3.52270436e-03],\n","        [-3.82305123e-03, -1.52532756e-03, -2.68875086e-03, ...,\n","         -1.99946295e-03,  4.39594453e-03, -3.53048462e-03]],\n","\n","       ...,\n","\n","       [[ 3.30475654e-04,  1.44331323e-04, -1.80458257e-04, ...,\n","          5.72627650e-05,  1.89925195e-04, -5.20303838e-05],\n","        [ 5.09623904e-04,  3.48105474e-04, -4.77785798e-04, ...,\n","         -8.50621291e-05,  1.32303239e-05, -8.84711626e-05],\n","        [ 5.80565771e-04,  4.48931940e-04, -4.12939989e-04, ...,\n","         -6.96637217e-05, -1.62373733e-04, -1.84223813e-04],\n","        ...,\n","        [-3.85361235e-03, -1.57381396e-03, -2.50701630e-03, ...,\n","         -1.98210380e-03,  4.03300347e-03, -3.43096140e-03],\n","        [-3.87053308e-03, -1.58310018e-03, -2.54630018e-03, ...,\n","         -2.04128842e-03,  4.14138380e-03, -3.44422949e-03],\n","        [-3.88268055e-03, -1.58653862e-03, -2.58124154e-03, ...,\n","         -2.09448929e-03,  4.23500827e-03, -3.45635298e-03]],\n","\n","       [[ 3.30475654e-04,  1.44331323e-04, -1.80458257e-04, ...,\n","          5.72627650e-05,  1.89925195e-04, -5.20303838e-05],\n","        [ 5.74887847e-04,  4.25137368e-05, -5.59745706e-04, ...,\n","          1.24313563e-04,  3.05329100e-04, -1.56880880e-04],\n","        [ 3.86515341e-04, -1.85260636e-04, -7.64617056e-04, ...,\n","          1.69520878e-04,  5.21297916e-04,  4.00705430e-05],\n","        ...,\n","        [-3.69630614e-03, -1.48712366e-03, -2.50853947e-03, ...,\n","         -1.90668076e-03,  4.02431097e-03, -3.46780126e-03],\n","        [-3.73518839e-03, -1.51418906e-03, -2.54805223e-03, ...,\n","         -1.97469490e-03,  4.13727900e-03, -3.48723424e-03],\n","        [-3.76614858e-03, -1.53195346e-03, -2.58350885e-03, ...,\n","         -2.03603506e-03,  4.23505949e-03, -3.50169349e-03]],\n","\n","       [[ 3.30475654e-04,  1.44331323e-04, -1.80458257e-04, ...,\n","          5.72627650e-05,  1.89925195e-04, -5.20303838e-05],\n","        [ 4.55575791e-04,  2.02695854e-04, -9.78342214e-05, ...,\n","         -6.84314800e-05, -4.91900355e-05,  1.04632796e-04],\n","        [ 5.55021688e-04,  8.10286656e-05, -1.77402311e-04, ...,\n","         -3.11149604e-04, -1.82627045e-04,  2.32460123e-04],\n","        ...,\n","        [-3.76038672e-03, -1.56764162e-03, -2.77011096e-03, ...,\n","         -1.93531031e-03,  4.21295688e-03, -3.47914104e-03],\n","        [-3.77690047e-03, -1.58574770e-03, -2.78018368e-03, ...,\n","         -2.00373237e-03,  4.29895986e-03, -3.49055580e-03],\n","        [-3.79037461e-03, -1.59500970e-03, -2.78849131e-03, ...,\n","         -2.06512026e-03,  4.37255343e-03, -3.49974562e-03]]],\n","      dtype=float32)>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2TiujFPpOMwN","executionInfo":{"status":"ok","timestamp":1642433865735,"user_tz":-540,"elapsed":20,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"458b7051-6853-47f7-ff90-79a52f123c43"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"text_generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       multiple                  3072256   \n","                                                                 \n"," lstm (LSTM)                 multiple                  5246976   \n","                                                                 \n"," lstm_1 (LSTM)               multiple                  8392704   \n","                                                                 \n"," dense (Dense)               multiple                  12301025  \n","                                                                 \n","=================================================================\n","Total params: 29,012,961\n","Trainable params: 29,012,961\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# optimizer는 loss를 최소화하는 w(가중치)를 찾는 것 (cs231n에 나온 내용)\n","optimizer = tf.keras.optimizers.Adam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","# model.compile(loss, optimizer, metrics)\n","model.compile(loss=loss, optimizer=optimizer)\n","model.fit(train_dataset, validation_data = test_dataset, epochs=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1MNLVN-fOWAc","outputId":"525a0b93-e36c-4aac-c906-cddc766f9fc6","executionInfo":{"status":"ok","timestamp":1642436281391,"user_tz":-540,"elapsed":2415667,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","496/496 [==============================] - 214s 426ms/step - loss: 1.6324 - val_loss: 1.5506\n","Epoch 2/10\n","496/496 [==============================] - 221s 446ms/step - loss: 1.3928 - val_loss: 1.3495\n","Epoch 3/10\n","496/496 [==============================] - 221s 445ms/step - loss: 1.3070 - val_loss: 1.3055\n","Epoch 4/10\n","496/496 [==============================] - 221s 445ms/step - loss: 1.2597 - val_loss: 1.2718\n","Epoch 5/10\n","496/496 [==============================] - 221s 446ms/step - loss: 1.2182 - val_loss: 1.2440\n","Epoch 6/10\n","496/496 [==============================] - 221s 446ms/step - loss: 1.1803 - val_loss: 1.2206\n","Epoch 7/10\n","496/496 [==============================] - 221s 445ms/step - loss: 1.1449 - val_loss: 1.2012\n","Epoch 8/10\n","496/496 [==============================] - 221s 445ms/step - loss: 1.1111 - val_loss: 1.1832\n","Epoch 9/10\n","496/496 [==============================] - 221s 446ms/step - loss: 1.0785 - val_loss: 1.1674\n","Epoch 10/10\n","496/496 [==============================] - 222s 446ms/step - loss: 1.0470 - val_loss: 1.1545\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f99114bbe90>"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["데이터가 커서 훈련하는 데 시간이 제법 걸릴 겁니다. 여유를 가지고 작업하시면 좋아요 :)"],"metadata":{"id":"VdopCMad3jQj"}},{"cell_type":"code","source":["def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n","    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","    end_token = tokenizer.word_index[\"<end>\"]\n","    \n","    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다.\n","    while True:\n","        predict = model(test_tensor) # 입력받은 문장의 텐서를 입력합니다.\n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다.\n","        \n","        # 우리 모델이 새롭계 예측한 단어를 입력 문장의 뒤에 붙여줍니다. \n","        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n","        \n","        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면 while 루프를 돌면서 다음 단어를 예측해야합니다.\n","        if predict_word.numpy()[0] == end_token:\n","            break\n","        if test_tensor.shape[1] >= max_len:\n","            break\n","    \n","    generated = \"\"\n","    \n","    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다.\n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","    \n","    return generated # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."],"metadata":{"id":"gG7fYA1fZszQ","executionInfo":{"status":"ok","timestamp":1642436281392,"user_tz":-540,"elapsed":47,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"],"metadata":{"id":"o2GGrewx3h_C","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1642436281393,"user_tz":-540,"elapsed":45,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"dc444344-fb3d-449d-e1fb-820fa352f4a8"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> i love you , i m a survivor <end> '"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> if you\", max_len=20)"],"metadata":{"id":"fcwxtqXw87nk","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1642436281395,"user_tz":-540,"elapsed":41,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"ca207b83-e2b5-40e7-a693-2f563c856504"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> if you re a little girl , you know it <end> '"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> i wonder\", max_len=20)\n"],"metadata":{"id":"oNyTgbXnrYRJ","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1642436281396,"user_tz":-540,"elapsed":40,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"3799471d-417c-4775-8154-b3d7065e8597"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> i wonder why you know <end> '"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> i wanted\", max_len=20)"],"metadata":{"id":"WM2zOGc7ra-u","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1642436281397,"user_tz":-540,"elapsed":39,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"46658c32-33b4-4625-d6fb-34067377ef92"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> i wanted to be a little selfish <end> '"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> she loves\", max_len=20)"],"metadata":{"id":"Auer_HFpreGV","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1642436281398,"user_tz":-540,"elapsed":38,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"9d27053e-4733-49a5-a758-68b34a39367a"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> she loves me <end> '"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> please give \", max_len=20)"],"metadata":{"id":"aoLQuKWDriT7","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1642436282345,"user_tz":-540,"elapsed":982,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"8ee2ccab-f7d6-4adc-fb8d-63f90c3f24d7"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> please give me a million reasons <end> '"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> if you want \", max_len=20)"],"metadata":{"id":"j7X7zOGtrkPS","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1642436282349,"user_tz":-540,"elapsed":39,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"d7cf1a2c-5f38-43a9-f4ab-1f8adc3d8cbb"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> if you want to be your lover <end> '"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> say \", max_len=20)"],"metadata":{"id":"c3EpQvsPrmhn","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1642436282350,"user_tz":-540,"elapsed":37,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}},"outputId":"efd0fc78-d56e-4d5b-a233-780f2d625ab5"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> say it s time <end> '"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["> Q4. 모델이 생성한 가사 한 줄을 제출하세요.<br><br>\n","          i wonder why you know"],"metadata":{"id":"BvpXaf1o3nMz"}},{"cell_type":"markdown","source":["# 회고"],"metadata":{"id":"k2OTgQEJjNp0"}},{"cell_type":"markdown","source":["- ***이번 프로젝트에서 어려웠던 점,***<br>\n","\n","    `def tokenize(corpus)` 함수의 코드에서 tensor객체가 아닐때 부터 tensor라고 변수명이 지정되어있어서 많이 헷갈렸다. 그리고 validation을 왜 `train_test_split`으로 나누었는지 이해가 되지 않았다... 어떻게 잘 마무리했지만 헷갈릴 법한 소지가 너무 많지 않았나 생각한다.\n","    `model.fit(validation_split=0.2)` 이런식으로 기본 코드가 짜여있었더라면 좋았을 것 같다. 코드가 너무 길고 아직은 내가 파이썬에 능숙하진 못해서 코드를 변경시키려고 했으나 엄두가 나지 않아서 그렇게 하진 못했다.\n","\n","    그리고 지문중에 토큰 15개 이상의 문장을 삭제하라는 말이 있었는데, 15개 이상 문장을 삭제했을때, train 데이터셋의 갯수가 124960개를 넘어서 결국 13개 이상 문장을 삭제했어야 했다. 아이펠에서 점검이 필요한 사항이라고 생각한다."],"metadata":{"id":"xpqFS0RrjRpv"}},{"cell_type":"markdown","source":["- ***프로젝트를 진행하면서 알아낸 점 혹은 아직 모호한 점.***\n","\n","    토큰 15개 이상인 문장을 삭제하라고 해서, `def tokenize()`안에서 문장을 토큰화 한 다음에 만들어진 텐서를 아래와 같이 삭제를 시도하였다."],"metadata":{"id":"SJCirr2rl_Lt"}},{"cell_type":"code","source":["for ten in tensor:\n","    if len(ten) >= 15:\n","        del ten"],"metadata":{"id":"yHgcST5En2GF","executionInfo":{"status":"ok","timestamp":1642436282351,"user_tz":-540,"elapsed":36,"user":{"displayName":"김선아","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00416077006155103880"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["하지만, 텐서는 이렇게 `del`로 삭제가 불가능하였고, ndarray로 변환시켜서 삭제한 뒤 다시 텐서로 변형을 해 주어야한다고 블로그 검색을 통해 알게되었다. 하지만, 텐서를 ndarray로 변형시키는 함수인 `tensor.numpy()`와 `Tensor.eval()`, `Tensorflow.session()` 이 세 개를 다 적용해 보았으나 제대로 변형이 되질 않아서 계속 모두 실패하였다.\n","\n","오기가 생겨서 6시간동안 도전해 보았으나 실패... \n","이후에 다시 보니, 패딩을 넣기 전엔 `tokenizer.fit_on_texts(corpus)`에서 리스트를 반환했기 때문에, ndarray도 아니었고, tensor도 아니었다. 그래서 `tensor.numpy()`코드가 자꾸 에러가 났던 것이었다.\n","그 코드로 다시 또 시도해 보았으나, 이번엔 `del`에서 오류가 났다. 리스트의 요소를 삭제할 수는 있지만, 리스트 자체를 삭제하는 것이 아니기 때문이었다... (그런데 `del List명`이면 리스트 자체를 삭제 가능하다고 해서 조금 헷갈린다. for문에서 주어진 것이 바로 할당된 값이 아니어서 그런것인지..?)\n","\n","결국 원래 있던 코드인, `sentence.split()`를 통해 토큰화 하기 전 문장을 공백으로 만들어 주는 함수를 사용해서 해결하였다. 아직도 토큰화를 하고 나서 토큰의 갯수로 할 수 있다면 좋았을거라는 아쉬움이 든다."],"metadata":{"id":"uA9VaNpdoHsm"}},{"cell_type":"markdown","source":["- **루브릭 평가 지표를 맞추기 위해 시도한 것들**.\n","\n",">1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?<br>\n","        동작됨\n","    \n",">2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?<br>\n","        특수문자 제거, 토크나이저 생성, 패딩처리 모두 완료되었다.\n","    \n","> 3. 텍스트 생성모델이 안정적으로 학습되었는가?<br>\n","        학습됨"],"metadata":{"id":"KPoKQ4tBmNUQ"}},{"cell_type":"markdown","source":["- ***만약에 루브릭 평가 관련 지표를 달성 하지 못했을 때, 이유에 관한 추정.***\n","\n","    "],"metadata":{"id":"Nzvd3OucmQpS"}},{"cell_type":"markdown","source":["- ***자기 다짐***\n","\n","    파이썬 기본적인 문법에 대해서 좀 더 공부를 해야겠다는 생각이 들었다. 기본 문법에서 헷갈리니,코드 이해하는데 많은 시간이 소요되었다.."],"metadata":{"id":"E6K4q5ehmM0L"}}]}